{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "452993ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147046/560074825.py:53: FutureWarning: `get_analytical_jacobian` was part of PyTorch's private API and not meant to be exposed. We are deprecating it and it will be removed in a future version of PyTorch. If you have a specific use for this or feature request for this to be a stable API, please file us an issue at https://github.com/pytorch/pytorch/issues/new\n",
      "  (J_flat,), reentrant, sizes_ok, types_ok = get_analytical_jacobian(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 8836])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147046/560074825.py:70: FutureWarning: `get_numerical_jacobian` was part of PyTorch's private API and not meant to be exposed. We are deprecating it and it will be removed in a future version of PyTorch. If you have a specific use for this or feature request for this to be a stable API, please file us an issue at https://github.com/pytorch/pytorch/issues/new\n",
      "  (J_flat,) = get_numerical_jacobian(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 8836])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147046/560074825.py:53: FutureWarning: `get_analytical_jacobian` was part of PyTorch's private API and not meant to be exposed. We are deprecating it and it will be removed in a future version of PyTorch. If you have a specific use for this or feature request for this to be a stable API, please file us an issue at https://github.com/pytorch/pytorch/issues/new\n",
      "  (J_flat,), reentrant, sizes_ok, types_ok = get_analytical_jacobian(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 8836])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147046/560074825.py:70: FutureWarning: `get_numerical_jacobian` was part of PyTorch's private API and not meant to be exposed. We are deprecating it and it will be removed in a future version of PyTorch. If you have a specific use for this or feature request for this to be a stable API, please file us an issue at https://github.com/pytorch/pytorch/issues/new\n",
      "  (J_flat,) = get_numerical_jacobian(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 8836])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from torch.autograd.gradcheck import (\n",
    "    get_analytical_jacobian,\n",
    "    get_numerical_jacobian,\n",
    ")\n",
    "\n",
    "from dxtb import GFN1_XTB as par\n",
    "from dxtb import Calculator, OutputHandler, labels\n",
    "from dxtb._src.typing import DD, Callable, Tensor\n",
    "from dxtb.config import ConfigCache\n",
    "\n",
    "from samples import samples\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "def gradchecker(\n",
    "    dtype: torch.dtype, name: str, scf_mode: str, scp_mode: str\n",
    ") -> tuple[Callable[[Tensor], Tensor], Tensor]:\n",
    "    \"\"\"Prepare gradient check from `torch.autograd`.\"\"\"\n",
    "    dd: DD = {\"dtype\": dtype, \"device\": DEVICE}\n",
    "\n",
    "    sample = samples[name]\n",
    "    numbers = sample[\"numbers\"].to(DEVICE)\n",
    "    positions = sample[\"positions\"].to(**dd)\n",
    "\n",
    "    opts = {\n",
    "        \"scf_mode\": scf_mode,\n",
    "        \"scp_mode\": scp_mode,\n",
    "    }\n",
    "\n",
    "    calc = Calculator(numbers, par, **dd, opts=opts)\n",
    "    calc.opts.cache = ConfigCache(enabled=False, fock=True)\n",
    "    OutputHandler.verbosity = 0\n",
    "\n",
    "    # variables to be differentiated\n",
    "    pos = positions.clone().requires_grad_(True)\n",
    "\n",
    "    def func(p: Tensor) -> Tensor:\n",
    "        _ = calc.get_energy(p)  # triggers Fock matrix computation\n",
    "        return calc.cache[\"fock\"]\n",
    "\n",
    "    return func, pos\n",
    "\n",
    "def analytical_jacobian(fn, inputs):\n",
    "    # wrap input in a 1‑tuple, flatten output to 1‑D\n",
    "    inputs_tup = (inputs,)\n",
    "    y = fn(inputs)\n",
    "    y_flat = y.reshape(-1)    # shape [M*N]\n",
    "    # returns a tuple of Jacobians—one per input\n",
    "    (J_flat,), reentrant, sizes_ok, types_ok = get_analytical_jacobian(\n",
    "        inputs_tup,\n",
    "        y_flat,\n",
    "        nondet_tol=0.0,\n",
    "        grad_out=1.0,\n",
    "    )\n",
    "    # J_flat shape: (inputs.numel(), y_flat.numel())\n",
    "    return J_flat\n",
    "\n",
    "def numerical_jacobian(fn, inputs, eps=1e-6):\n",
    "    # we need a function that takes a tuple of inputs\n",
    "    # and returns a flat (1‑D) output\n",
    "    def flat_fn(inp_tuple):\n",
    "        x = inp_tuple[0]\n",
    "        y = fn(x)\n",
    "        return y.reshape(-1)\n",
    "    # get_numerical_jacobian returns one Jacobian per input\n",
    "    (J_flat,) = get_numerical_jacobian(\n",
    "        flat_fn,\n",
    "        inputs,   # single Tensor; internals call _as_tuple on it\n",
    "        eps=eps,\n",
    "    )\n",
    "    # J_flat shape: (inputs.numel(), y_flat.numel())\n",
    "    return J_flat\n",
    "\n",
    "\n",
    "#### Testing ####\n",
    "SCP_MODE = \"potential\"\n",
    "\n",
    "NAME = \"MB16_43_01\" # Diff of 1e-1 for implicit\n",
    "NAME = \"SiH4\" # Diff of 2e-2 \n",
    "NAME = \"LYS_xao\" # \n",
    "\n",
    "func_impl, pos_impl = gradchecker(torch.float64, NAME, \"implicit\", SCP_MODE)\n",
    "func_full, pos_full = gradchecker(torch.float64, NAME, \"full\", SCP_MODE)\n",
    "\n",
    "# compute slow/full analytical and numerical\n",
    "J_an_impl = analytical_jacobian(func_impl, pos_impl); print(J_an_impl.shape)\n",
    "J_num_impl = numerical_jacobian(func_impl, pos_impl); print(J_num_impl.shape)\n",
    "J_an_full = analytical_jacobian(func_full, pos_full); print(J_an_full.shape)\n",
    "J_num_full = numerical_jacobian(func_full, pos_full); print(J_num_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f98bbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff gradients (implicit): 1.61e-01\n",
      "Max diff gradients (full): 2.13e-08\n"
     ]
    }
   ],
   "source": [
    "max_diff_impl = torch.max(torch.abs(J_an_impl - J_num_impl))\n",
    "print(f\"Max diff gradients (implicit): {max_diff_impl:.2e}\")\n",
    "max_diff_full = torch.max(torch.abs(J_an_full - J_num_full))\n",
    "print(f\"Max diff gradients (full): {max_diff_full:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da9788",
   "metadata": {},
   "source": [
    "### (Dependent on the sample) the gradients of the implicit version have differences of up to 1e-1!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dxtb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
