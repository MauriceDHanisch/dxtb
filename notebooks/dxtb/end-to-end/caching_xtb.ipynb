{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads_with_cache: [tensor([[-0.0096, -0.5187],\n",
      "        [ 0.1414, -0.2812],\n",
      "        [-0.0567,  0.2288]]), tensor([[ 0.0724, -0.2029],\n",
      "        [ 0.2140, -0.0736],\n",
      "        [-0.0720,  0.1057]])]\n",
      "grads_without_cache: [tensor([[-0.0316, -0.9845],\n",
      "        [ 0.2710, -0.5067],\n",
      "        [-0.1156,  0.4451]]), tensor([[ 0.1361, -0.3991],\n",
      "        [ 0.4105, -0.1482],\n",
      "        [-0.1392,  0.1871]])]\n",
      "Gradients for molecule 0 differ with and without caching.\n",
      "Gradients for molecule 1 differ with and without caching.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "# Simulating a differentiable feature computation\n",
    "def differentiable_feature_computation(atomic_numbers, positions):\n",
    "    # Example F, P, S, H are now matrices, derived from the positions\n",
    "    F = torch.matmul(positions, positions.T)  # Example F feature (matrix of dot products)\n",
    "    P = positions @ positions.T               # Example P feature (same as F in this case)\n",
    "    S = positions * positions                 # Example S feature (element-wise product)\n",
    "    H = torch.mean(positions, dim=0, keepdim=True) @ positions.T  # Example H feature (mean with matmul)\n",
    "    return F, P, S, H\n",
    "\n",
    "# Class to cache FPSH features\n",
    "class MolFeatureWithCache:\n",
    "    def __init__(self, atomic_numbers, positions):\n",
    "        self.atomic_numbers = atomic_numbers\n",
    "        self.positions = positions.requires_grad_()  # Positions need gradients\n",
    "        self._cached_fpsh = None  # This will store the cached FPSH matrices\n",
    "\n",
    "    def compute_or_cache_features(self, cache_enabled=True):\n",
    "        if self._cached_fpsh is None or not cache_enabled:\n",
    "            # Compute FPSH features for the first time or if caching is disabled\n",
    "            for _ in range(10000):\n",
    "                F, P, S, H = differentiable_feature_computation(self.atomic_numbers, self.positions)\n",
    "            self._cached_fpsh = (F, P, S, H)  # Cache the computed features\n",
    "        return self._cached_fpsh\n",
    "\n",
    "    def clear_cache(self):\n",
    "        # Clears the cache (only if positions change)\n",
    "        self._cached_fpsh = None\n",
    "\n",
    "# A simple ML model that takes FPSH features as input and predicts scalar energy\n",
    "class SimpleMLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLModel, self).__init__()\n",
    "        self.fc = nn.Linear(27, 1)  # Assume flattened FPSH concatenates into a vector of size 9 * 4\n",
    "\n",
    "    def forward(self, F, P, S, H):\n",
    "        F_flat = F.view(-1)  # Flatten the F matrix\n",
    "        P_flat = P.view(-1)  # Flatten the P matrix\n",
    "        S_flat = S.view(-1)  # Flatten the S matrix\n",
    "        H_flat = H.view(-1)  # Flatten the H matrix\n",
    "\n",
    "        # Concatenate all features into one long vector\n",
    "        features = torch.cat([F_flat, P_flat, S_flat, H_flat], dim=0)\n",
    "        energy = self.fc(features.unsqueeze(0))  # Add batch dimension for the Linear layer\n",
    "        return energy\n",
    "\n",
    "# Create a toy dataset of atomic positions and atomic numbers\n",
    "def create_toy_dataset(n_molecules):\n",
    "    dataset = []\n",
    "    for i in range(n_molecules):\n",
    "        # Random atomic positions for 3 atoms in 2D space\n",
    "        positions = torch.randn(3, 2)\n",
    "        # Random atomic numbers (just for the sake of it, but not used here)\n",
    "        atomic_numbers = torch.randint(1, 10, (3,))\n",
    "        mol_feature = MolFeatureWithCache(atomic_numbers, positions)\n",
    "        dataset.append(mol_feature)\n",
    "    return dataset\n",
    "\n",
    "# Training loop\n",
    "def train(model, dataset, epochs=1, learning_rate=1e-3, cache_enabled=True):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    positions_grads = []\n",
    "    for epoch in range(epochs):\n",
    "        for mol_feature in dataset:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get cached or compute FPSH features\n",
    "            F, P, S, H = mol_feature.compute_or_cache_features(cache_enabled=cache_enabled)\n",
    "\n",
    "            # Forward pass through the ML model to compute energy\n",
    "            energy = model(F, P, S, H)\n",
    "\n",
    "            # Loss function: Minimize energy\n",
    "            loss = torch.sum(energy ** 2)\n",
    "\n",
    "            # Backpropagate energy to compute dE/dW\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            # Store the gradients of the positions\n",
    "            positions_grads.append(copy.deepcopy(mol_feature.positions.grad.clone()))\n",
    "\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "    return positions_grads\n",
    "\n",
    "# Compare gradients with and without caching\n",
    "def compare_grads(grads_with_cache, grads_without_cache):\n",
    "    for i, (grad_cache, grad_no_cache) in enumerate(zip(grads_with_cache, grads_without_cache)):\n",
    "        if torch.allclose(grad_cache, grad_no_cache):\n",
    "            print(f\"Gradients for molecule {i} are the same with and without caching.\")\n",
    "        else:\n",
    "            print(f\"Gradients for molecule {i} differ with and without caching.\")\n",
    "\n",
    "# Main function to test the example\n",
    "def main():\n",
    "    # Create toy dataset with 5 molecules\n",
    "    dataset = create_toy_dataset(n_molecules=2)\n",
    "\n",
    "    # Initialize the ML model\n",
    "    model = SimpleMLModel()\n",
    "\n",
    "    grads_with_cache = train(model, dataset, epochs=1, cache_enabled=True)\n",
    "\n",
    "    # Train the model without caching\n",
    "    grads_without_cache = train(model, dataset, epochs=1, cache_enabled=False)\n",
    "\n",
    "    print(f\"grads_with_cache: {grads_with_cache}\")\n",
    "    print(f\"grads_without_cache: {grads_without_cache}\")\n",
    "\n",
    "    # Compare the gradients\n",
    "    compare_grads(grads_with_cache, grads_without_cache)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbnet_dxtb_libpath",
   "language": "python",
   "name": "orbnet_dxtb_libpath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
