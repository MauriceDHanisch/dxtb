{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import dxtb\n",
    "from dxtb.typing import DD\n",
    "from dxtb.config import ConfigCache\n",
    "from dxtb import OutputHandler\n",
    "from tblite.interface import Calculator\n",
    "\n",
    "\n",
    "dd: DD = {\"dtype\": torch.double, \"device\": torch.device(\"cpu\")}\n",
    "\n",
    "# LiH\n",
    "numbers = torch.tensor([3, 1], device=dd[\"device\"])\n",
    "positions = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 1.5]], **dd) # ** to use dd as kwargs \n",
    "\n",
    "# numbers = torch.tensor([6, 6, 7, 7, 1, 1, 1, 1, 1, 1, 8, 8,], device=dd[\"device\"])\n",
    "# positions = torch.tensor([\n",
    "#                 [-3.81469488143921, +0.09993441402912, 0.00000000000000],\n",
    "#                 [+3.81469488143921, -0.09993441402912, 0.00000000000000],\n",
    "#                 [-2.66030049324036, -2.15898251533508, 0.00000000000000],\n",
    "#                 [+2.66030049324036, +2.15898251533508, 0.00000000000000],\n",
    "#                 [-0.73178529739380, -2.28237795829773, 0.00000000000000],\n",
    "#                 [-5.89039325714111, -0.02589114569128, 0.00000000000000],\n",
    "#                 [-3.71254944801331, -3.73605775833130, 0.00000000000000],\n",
    "#                 [+3.71254944801331, +3.73605775833130, 0.00000000000000],\n",
    "#                 [+0.73178529739380, +2.28237795829773, 0.00000000000000],\n",
    "#                 [+5.89039325714111, +0.02589114569128, 0.00000000000000],\n",
    "#                 [-2.74426102638245, +2.16115570068359, 0.00000000000000],\n",
    "#                 [+2.74426102638245, -2.16115570068359, 0.00000000000000],\n",
    "#                 ], **dd) # ** to use dd as kwargs\n",
    "\n",
    "pos = positions.clone().requires_grad_(True)\n",
    "\n",
    "# instantiate a dxtb calculator\n",
    "cache_config = ConfigCache(enabled=True, density=True, fock=True, overlap=True, hcore=True)\n",
    "cache_config = ConfigCache(enabled=True, density=True, fock=True, overlap=True, hcore=True, mo_energies=True, coefficients=True)\n",
    "calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, CACHE_STORE_DENSITY=True, **dd)\n",
    "calc.opts.cache = cache_config\n",
    "OutputHandler.verbosity = 0\n",
    "\n",
    "# instantiate a tblite calculator\n",
    "calc_tblite = Calculator(\n",
    "    method = \"GFN1-xTB\",\n",
    "    numbers = numbers.numpy(),\n",
    "    positions = positions.numpy(),\n",
    "    uhf=None\n",
    ")\n",
    "\n",
    "calc_tblite.set(\"save-integrals\", 1)\n",
    "calc_tblite.set(\"verbosity\", 0)\n",
    "res = calc_tblite.singlepoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if output dxtb and tblite are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qcm_ml.features.xtb import _get_permutation_map_closedshell_convention, apply_perm_map\n",
    "from dxtb.integrals.wrappers import overlap\n",
    "\n",
    "perm_map_dxtb = _get_permutation_map_closedshell_convention(numbers.numpy(), \"dxtb\")\n",
    "perm_map_tblite = _get_permutation_map_closedshell_convention(numbers.numpy(), \"tblite\")\n",
    "\n",
    "p_dxtb = apply_perm_map(calc.get_density(pos), perm_map_dxtb, positions.numpy())\n",
    "p_tblite = apply_perm_map(res[\"density-matrix\"], perm_map_tblite, positions.numpy())\n",
    "\n",
    "# E_dxtb, C_dxtb, S_dxtb = calc.get_mo_energies(pos), calc.get_coefficients(pos), overlap(numbers, pos, dxtb.GFN1_XTB)\n",
    "# F_dxtb = apply_perm_map(torch.einsum('ij,jk,kl,lm->im', S_dxtb, C_dxtb, torch.diag(E_dxtb), torch.inverse(C_dxtb)), perm_map_dxtb, positions.numpy())\n",
    "F_dxtb = apply_perm_map(calc.cache[\"fock\"], perm_map_dxtb, positions.numpy())\n",
    "\n",
    "E_tblite, C_tblite, S_tblite = res[\"orbital-energies\"], res[\"orbital-coefficients\"], res[\"overlap-matrix\"]\n",
    "F_tblite = apply_perm_map(np.einsum('ij,jk,kl,lm->im', S_tblite, C_tblite, np.diag(E_tblite), np.linalg.inv(C_tblite)), perm_map_tblite, positions.numpy())\n",
    "\n",
    "assert torch.allclose(p_dxtb, torch.tensor(p_tblite, **dd), atol=1e-3)\n",
    "assert torch.allclose(F_dxtb, torch.tensor(F_tblite, **dd), atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if 5 point stencil and manual jacobian are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qcm_ml.features.xtb import get_jacobian, get_2body_grads\n",
    "\n",
    "\n",
    "P_jac_dxtb = get_jacobian(p_dxtb, pos)\n",
    "F_jac_dxtb = get_jacobian(F_dxtb, pos)\n",
    "\n",
    "two_body_grads = get_2body_grads(numbers.numpy(), positions.numpy())\n",
    "P_jac_tblite = two_body_grads[1]\n",
    "F_jac_tblite = two_body_grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff in P_jac: 3.13e-02 with max value in P_jac:  1.63e-01\n",
      "Max diff in F_jac: 7.62e-03 with max value in F_jac:  1.46e-01\n",
      "P_jac_dxtb slice: tensor([[[[ 4.8669e-17, -1.4612e-17,  8.9727e-02],\n",
      "          [-4.8669e-17,  1.4612e-17, -8.9727e-02]]]], dtype=torch.float64,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "F_jac_dxtb slice: tensor([[[[-2.1259e-17,  6.0837e-18,  1.2514e-03],\n",
      "          [ 2.1259e-17, -6.0837e-18, -1.2514e-03]]]], dtype=torch.float64,\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max diff in P_jac: {torch.max(torch.abs(P_jac_dxtb - torch.tensor(P_jac_tblite, **dd))):.2e}\"\n",
    "      f\" with max value in P_jac: {torch.max(torch.abs(P_jac_dxtb)): .2e}\")\n",
    "print(f\"Max diff in F_jac: {torch.max(torch.abs(F_jac_dxtb - torch.tensor(F_jac_tblite, **dd))):.2e}\"\n",
    "        f\" with max value in F_jac: {torch.max(torch.abs(F_jac_dxtb)): .2e}\")\n",
    "\n",
    "print(f\"P_jac_dxtb slice: {P_jac_dxtb[:1, :1, :2, :]}\")\n",
    "print(f\"F_jac_dxtb slice: {F_jac_dxtb[:1, :1, :2, :]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Output 0 of the user-provided function is independent of input 0. This is not allowed in strict mode.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     F_dxtb \u001b[38;5;241m=\u001b[39m apply_perm_map(torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mij,jk,kl,lm->im\u001b[39m\u001b[38;5;124m'\u001b[39m, S_dxtb, C_dxtb, torch\u001b[38;5;241m.\u001b[39mdiag(E_dxtb), torch\u001b[38;5;241m.\u001b[39minverse(C_dxtb)), perm_map_dxtb, positions\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_dxtb\n\u001b[0;32m----> 8\u001b[0m P_jac_auto \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m F_jac_auto \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mjacobian(get_f, pos, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/orbnet_tblite/lib/python3.9/site-packages/torch/autograd/functional.py:702\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m strict:\n\u001b[1;32m    699\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m of the user-provided function is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    700\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindependent of input \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. This is not allowed in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, el_idx))\n\u001b[0;32m--> 702\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    703\u001b[0m             jac_i_el\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mzeros_like(inp_el))\n\u001b[1;32m    705\u001b[0m jacobian \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mtuple\u001b[39m(torch\u001b[38;5;241m.\u001b[39mstack(jac_i_el, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize()  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    706\u001b[0m              \u001b[38;5;241m+\u001b[39m inputs[el_idx]\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;28;01mfor\u001b[39;00m (el_idx, jac_i_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(jac_i)), )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Output 0 of the user-provided function is independent of input 0. This is not allowed in strict mode."
     ]
    }
   ],
   "source": [
    "def get_p(pos):\n",
    "    return apply_perm_map(calc.get_density(pos), perm_map_dxtb, positions.numpy())\n",
    "def get_f(pos):\n",
    "    E_dxtb, C_dxtb, S_dxtb = calc.get_mo_energies(pos), calc.get_coefficients(pos), overlap(numbers, pos, dxtb.GFN1_XTB)\n",
    "    F_dxtb = apply_perm_map(torch.einsum('ij,jk,kl,lm->im', S_dxtb, C_dxtb, torch.diag(E_dxtb), torch.inverse(C_dxtb)), perm_map_dxtb, positions.numpy())\n",
    "    return F_dxtb\n",
    "\n",
    "P_jac_auto = torch.autograd.functional.jacobian(get_p, pos, strict=True)\n",
    "F_jac_auto = torch.autograd.functional.jacobian(get_f, pos, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff in P_jac:  1.63e-01 with max value in P_jac:  0.00e+00\n",
      "Max diff in F_jac:  1.08e-01 with max value in F_jac:  7.87e-02\n",
      "P_jac_auto slice: tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.]]]], dtype=torch.float64)\n",
      "F_jac_auto slice: tensor([[[[ 0.0000,  0.0000, -0.0233],\n",
      "          [ 0.0000,  0.0000,  0.0233]]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max diff in P_jac: {torch.max(torch.abs(P_jac_auto - P_jac_dxtb)): .2e}\"\n",
    "      f\" with max value in P_jac: {torch.max(torch.abs(P_jac_auto)): .2e}\")\n",
    "print(f\"Max diff in F_jac: {torch.max(torch.abs(F_jac_auto - F_jac_dxtb)): .2e}\"\n",
    "        f\" with max value in F_jac: {torch.max(torch.abs(F_jac_auto)): .2e}\")\n",
    "\n",
    "print(f\"P_jac_auto slice: {P_jac_auto[:1, :1, :2, :]}\")\n",
    "print(f\"F_jac_auto slice: {F_jac_auto[:1, :1, :2, :]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbnet_tblite_libpath",
   "language": "python",
   "name": "orbnet_tblite_libpath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
