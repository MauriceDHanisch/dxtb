{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import dxtb\n",
    "from dxtb.typing import DD\n",
    "from dxtb.config import ConfigCache\n",
    "from dxtb import OutputHandler\n",
    "from tblite.interface import Calculator\n",
    "\n",
    "\n",
    "dd: DD = {\"dtype\": torch.double, \"device\": torch.device(\"cpu\")}\n",
    "\n",
    "# LiH\n",
    "# numbers = torch.tensor([3, 1], device=dd[\"device\"])\n",
    "# positions = torch.tensor([[0.0, 0.0, 0.0], [0.0, 0.0, 1.5]], **dd) # ** to use dd as kwargs \n",
    "\n",
    "numbers = torch.tensor([6, 6, 7, 7, 1, 1, 1, 1, 1, 1, 8, 8,], device=dd[\"device\"])\n",
    "positions = torch.tensor([\n",
    "                [-3.81469488143921, +0.09993441402912, 0.00000000000000],\n",
    "                [+3.81469488143921, -0.09993441402912, 0.00000000000000],\n",
    "                [-2.66030049324036, -2.15898251533508, 0.00000000000000],\n",
    "                [+2.66030049324036, +2.15898251533508, 0.00000000000000],\n",
    "                [-0.73178529739380, -2.28237795829773, 0.00000000000000],\n",
    "                [-5.89039325714111, -0.02589114569128, 0.00000000000000],\n",
    "                [-3.71254944801331, -3.73605775833130, 0.00000000000000],\n",
    "                [+3.71254944801331, +3.73605775833130, 0.00000000000000],\n",
    "                [+0.73178529739380, +2.28237795829773, 0.00000000000000],\n",
    "                [+5.89039325714111, +0.02589114569128, 0.00000000000000],\n",
    "                [-2.74426102638245, +2.16115570068359, 0.00000000000000],\n",
    "                [+2.74426102638245, -2.16115570068359, 0.00000000000000],\n",
    "                ], **dd) # ** to use dd as kwargs\n",
    "\n",
    "pos = positions.clone().requires_grad_(True)\n",
    "\n",
    "# instantiate a dxtb calculator\n",
    "cache_config = ConfigCache(enabled=True, coefficients=True, mo_energies=True, density=True)\n",
    "cache_config = ConfigCache(enabled=True, coefficients=True, mo_energies=True, density=True, overlap=True)\n",
    "calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, CACHE_STORE_DENSITY=True, **dd)\n",
    "calc.opts.cache = cache_config\n",
    "OutputHandler.verbosity = 0\n",
    "\n",
    "# instantiate a tblite calculator\n",
    "calc_tblite = Calculator(\n",
    "    method = \"GFN1-xTB\",\n",
    "    numbers = numbers.numpy(),\n",
    "    positions = positions.numpy(),\n",
    "    uhf=None\n",
    ")\n",
    "\n",
    "calc_tblite.set(\"save-integrals\", 1)\n",
    "calc_tblite.set(\"verbosity\", 0)\n",
    "res = calc_tblite.singlepoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if output dxtb and tblite are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qcm_ml.features.xtb import _get_permutation_map_closedshell_convention, apply_perm_map\n",
    "\n",
    "perm_map_dxtb = _get_permutation_map_closedshell_convention(numbers.numpy(), \"dxtb\")\n",
    "perm_map_tblite = _get_permutation_map_closedshell_convention(numbers.numpy(), \"tblite\")\n",
    "\n",
    "s_dxtb = apply_perm_map(calc.integrals.build_overlap(pos), perm_map_dxtb, positions.numpy())\n",
    "s_tblite = apply_perm_map(res[\"overlap-matrix\"], perm_map_tblite, positions.numpy())\n",
    "\n",
    "h_dxtb = apply_perm_map(calc.integrals.build_hcore(pos), perm_map_dxtb, positions.numpy())\n",
    "h_tblite = apply_perm_map(res[\"hamiltonian-matrix\"], perm_map_tblite, positions.numpy())\n",
    "\n",
    "assert torch.allclose(s_dxtb, torch.tensor(s_tblite, **dd), atol=1e-6)\n",
    "assert torch.allclose(h_dxtb, torch.tensor(h_tblite, **dd), atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if 5 point stencil and manual jacobian are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qcm_ml.features.xtb import get_jacobian, get_2body_grads\n",
    "\n",
    "two_body_grads = get_2body_grads(numbers.numpy(), positions.numpy())\n",
    "\n",
    "s_jac_dxtb = get_jacobian(s_dxtb, pos)\n",
    "h_jac_dxtb = get_jacobian(h_dxtb, pos)\n",
    "\n",
    "s_jac_tblite = two_body_grads[2]\n",
    "h_jac_tblite = two_body_grads[3]\n",
    "\n",
    "assert torch.allclose(s_jac_dxtb, torch.tensor(s_jac_tblite, **dd), atol=1e-6)\n",
    "assert torch.allclose(h_jac_dxtb, torch.tensor(h_jac_tblite, **dd), atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if autograd.functional.jacobian and manual jacobian are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dxtb.integrals.wrappers import hcore, overlap\n",
    "\n",
    "def get_s(pos):\n",
    "    return apply_perm_map(overlap(numbers, pos, dxtb.GFN1_XTB), perm_map_dxtb, positions.numpy())\n",
    "def get_h(pos):\n",
    "    return apply_perm_map(hcore(numbers, pos, dxtb.GFN1_XTB), perm_map_dxtb, positions.numpy())\n",
    "\n",
    "s_jac_auto = torch.autograd.functional.jacobian(get_s, pos, strict=True)\n",
    "h_jac_auto = torch.autograd.functional.jacobian(get_h, pos, strict=True)\n",
    "\n",
    "assert torch.allclose(s_jac_auto, s_jac_dxtb, atol=1e-6)\n",
    "assert torch.allclose(h_jac_auto, h_jac_dxtb, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s(pos):\n",
    "    return apply_perm_map(calc.integrals.build_overlap(pos), perm_map_dxtb, positions.numpy())\n",
    "def get_h(pos):\n",
    "    return apply_perm_map(calc.integrals.build_hcore(pos), perm_map_dxtb, positions.numpy())\n",
    "\n",
    "# This does not work because it does not support the calc.integrals.build_*** method\n",
    "# s_jac_auto = torch.autograd.functional.jacobian(get_s, pos, strict=True)\n",
    "# h_jac_auto = torch.autograd.functional.jacobian(get_h, pos, strict=True)\n",
    "\n",
    "# Also vectorized is not supported as custom einsum function is used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbnet_tblite_libpath",
   "language": "python",
   "name": "orbnet_tblite_libpath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
