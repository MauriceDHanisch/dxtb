{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positions.shape (nbatch, nat, 3): torch.Size([2, 3, 3])\n",
      "numbers_list: [tensor([3, 1]), tensor([8, 1, 1])]\n",
      "positions_list: [tensor([[0., 0., 0.],\n",
      "        [0., 0., 1.]], dtype=torch.float64, grad_fn=<IndexBackward0>), tensor([[0., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 2.]], dtype=torch.float64, grad_fn=<IndexBackward0>)]\n",
      "padded_numbers: tensor([[3, 1, 0],\n",
      "        [8, 1, 1]])\n",
      "padded_positions: tensor([[[0., 0., 0.],\n",
      "         [0., 0., 1.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 1.],\n",
      "         [0., 0., 2.]]], dtype=torch.float64, grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dxtb\n",
    "from dxtb.typing import DD\n",
    "\n",
    "from qcm_ml.orbnet_equi.model import generate_xtb_matrices_dxtb\n",
    "\n",
    "def add_padding(numbers_list, positions_list, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of atomic numbers and positions tensors to match the longest tensor in the batch.\n",
    "    \n",
    "    Parameters:\n",
    "    - numbers_list (List[torch.Tensor]): List of 1D tensors with atomic numbers for each molecule.\n",
    "    - positions_list (List[torch.Tensor]): List of 2D tensors with atomic positions for each molecule.\n",
    "    - padding_value (int): Value used for padding (default: 0).\n",
    "    \n",
    "    Returns:\n",
    "    - padded_numbers (torch.Tensor): Padded atomic numbers, shape (nbatch, max_natoms).\n",
    "    - padded_positions (torch.Tensor): Padded atomic positions, shape (nbatch, max_natoms, 3).\n",
    "    \"\"\"\n",
    "    max_natoms = max(numbers.size(0) for numbers in numbers_list)\n",
    "    nbatch = len(numbers_list)\n",
    "    \n",
    "    # Initialize padded tensors with padding_value\n",
    "    padded_numbers = torch.full((nbatch, max_natoms), padding_value, dtype=numbers_list[0].dtype, device=numbers_list[0].device)\n",
    "    padded_positions = torch.full((nbatch, max_natoms, 3), padding_value, dtype=positions_list[0].dtype, device=positions_list[0].device)\n",
    "\n",
    "    for i in range(nbatch):\n",
    "        natoms = numbers_list[i].size(0)\n",
    "        padded_numbers[i, :natoms] = numbers_list[i]\n",
    "        padded_positions[i, :natoms, :] = positions_list[i]\n",
    "\n",
    "    return padded_numbers, padded_positions\n",
    "\n",
    "\n",
    "def remove_padding(numbers, positions, padding_value=0):\n",
    "    \"\"\"\n",
    "    Removes padding atoms and corresponding coordinates for batched data.\n",
    "    \n",
    "    Parameters:\n",
    "    - numbers (torch.Tensor): Tensor of atomic numbers with padding, shape (nbatch, natoms).\n",
    "    - positions (torch.Tensor): Tensor of atomic positions with padding, shape (nbatch, natoms, 3).\n",
    "    - padding_value (int): The padding value in `numbers` to remove (default: 0).\n",
    "    \n",
    "    Returns:\n",
    "    - cleaned_numbers (List[torch.Tensor]): List of tensors with non-padded atomic numbers for each batch.\n",
    "    - cleaned_positions (List[torch.Tensor]): List of tensors with non-padded atomic positions for each batch.\n",
    "    \"\"\"\n",
    "    cleaned_numbers = []\n",
    "    cleaned_positions = []\n",
    "    \n",
    "    for i in range(numbers.size(0)):  # Iterate over each batch\n",
    "        # Mask to identify non-padding atoms\n",
    "        mask = numbers[i] != padding_value\n",
    "        # Apply the mask to remove padding\n",
    "        cleaned_numbers.append(numbers[i][mask])\n",
    "        cleaned_positions.append(positions[i][mask])\n",
    "        \n",
    "    return cleaned_numbers, cleaned_positions\n",
    "\n",
    "\n",
    "dd: DD = {\"device\": torch.device(\"cpu\"), \"dtype\": torch.double}\n",
    "\n",
    "numbers = torch.tensor(\n",
    "    [\n",
    "        [3, 1, 0],\n",
    "        [8, 1, 1],\n",
    "    ],\n",
    "    device=dd[\"device\"],\n",
    ")\n",
    "positions = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 1.0],\n",
    "            [0.0, 0.0, 0.0],\n",
    "        ],\n",
    "        [\n",
    "            [0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 1.0],\n",
    "            [0.0, 0.0, 2.0],\n",
    "        ],\n",
    "    ],\n",
    "    **dd\n",
    ").requires_grad_(True)\n",
    "print(f\"positions.shape (nbatch, nat, 3): {positions.shape}\")\n",
    "\n",
    "charge = torch.tensor([0, 0], **dd)\n",
    "\n",
    "n_ls, pos_ls = remove_padding(numbers, positions)\n",
    "print(f\"numbers_list: {n_ls}\")\n",
    "print(f\"positions_list: {pos_ls}\")\n",
    "\n",
    "padded_numbers, padded_positions = add_padding(n_ls, pos_ls)\n",
    "print(f\"padded_numbers: {padded_numbers}\")\n",
    "print(f\"padded_positions: {padded_positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched vs normal calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy: tensor([ 0.0111, -4.8440], dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "energy0: 0.01105307556317131, energy1: -4.843954905460443\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Batched\n",
    "opts = {\"verbosity\": 0, \"batch_mode\": 1}\n",
    "calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, opts=opts, **dd)\n",
    "energy = calc.get_energy(positions, chrg=charge, **dd)\n",
    "# forces = calc.get_forces(positions, chrg=charge) # Does not work for batched calculations\n",
    "print(f\"energy: {energy}\")\n",
    "\n",
    "# Individual\n",
    "opts_ind = {\"verbosity\": 0, \"batch_mode\": 0}\n",
    "calc0 = dxtb.Calculator(n_ls[0], dxtb.GFN1_XTB, opts=opts_ind, **dd)\n",
    "calc1 = dxtb.Calculator(n_ls[1], dxtb.GFN1_XTB, opts=opts_ind, **dd)\n",
    "\n",
    "energy0 = calc0.get_energy(pos_ls[0])\n",
    "energy1 = calc1.get_energy(pos_ls[1])\n",
    "print(f\"energy0: {energy0}, energy1: {energy1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forces in batches with energy.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: tensor([[[     0.0000,      0.0000,      2.5666],\n",
      "         [    -0.0000,     -0.0000,     -2.5666],\n",
      "         [     0.0000,      0.0000,      0.0000]],\n",
      "\n",
      "        [[    -0.0000,     -0.0000,      2.8084],\n",
      "         [     0.0000,      0.0000,     -2.2184],\n",
      "         [     0.0000,      0.0000,     -0.5900]]], dtype=torch.float64)\n",
      "\n",
      "grad0: tensor([[     0.0000,      0.0000,     -2.5666],\n",
      "        [    -0.0000,     -0.0000,      2.5666]], dtype=torch.float64)\n",
      "grad1: tensor([[    -0.0000,      0.0000,     -2.8084],\n",
      "        [    -0.0000,     -0.0000,      2.2184],\n",
      "        [     0.0000,     -0.0000,      0.5900]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "grad = torch.autograd.grad(energy.sum(), positions, retain_graph=True)[0]\n",
    "\n",
    "grad0 = calc0.get_forces(pos_ls[0])\n",
    "grad1 = calc1.get_forces(pos_ls[1])\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "print(f\"grad: {grad}\")\n",
    "print()\n",
    "print(f\"grad0: {grad0}\")\n",
    "print(f\"grad1: {grad1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiate wrt to 2 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43menergy0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_ls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_ls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/orbnet_dxtb/lib/python3.9/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(energy0, [pos_ls[0], pos_ls[0]], retain_graph=True, allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbnet_dxtb_libpath",
   "language": "python",
   "name": "orbnet_dxtb_libpath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
