{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ed616db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of carbon atoms in alkane_9_carbons: 9\n",
      "Nb of atoms: 29\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import h5py\n",
    "N_Cs = 9\n",
    "\n",
    "with h5py.File('../dxtb/dxtb-gpu/gpu-cpu_analysis/rdkit/alkanes_data_500.hdf5', 'r') as f:\n",
    "    for mol_name, data in f.items():\n",
    "        if mol_name == f\"alkane_{N_Cs}_carbons\":\n",
    "            atomic_numbers = data['atomic_numbers'][:]\n",
    "            coordinates = data['coordinates'][:]\n",
    "\n",
    "print(f\"Number of carbon atoms in {mol_name}: {N_Cs}\")\n",
    "print(f\"Nb of atoms: {len(atomic_numbers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20907998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of carbon atoms in alkane_9_carbons: 9\n",
      "Nb of atoms: 29\n",
      "batch_size: 64\n"
     ]
    }
   ],
   "source": [
    "import dxtb\n",
    "from dxtb._src.typing import DD\n",
    "import torch\n",
    "from dxtb.config import ConfigCache\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "print(f\"Number of carbon atoms in {mol_name}: {N_Cs}\")\n",
    "print(f\"Nb of atoms: {len(atomic_numbers)}\")\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "\n",
    "dd = {\"device\": torch.device(\"cuda:0\"), \"dtype\": torch.float64}\n",
    "numbers = torch.tensor(atomic_numbers, device=dd[\"device\"], dtype=torch.int32)\n",
    "positions = torch.tensor(coordinates, **dd).requires_grad_()\n",
    "charges = torch.tensor(0.0, **dd)\n",
    "# numbers = torch.stack([numbers] * batch_size)\n",
    "# positions = torch.stack([positions] * batch_size).requires_grad_()\n",
    "# charges = torch.zeros((batch_size,), device=dd[\"device\"], dtype=dd[\"dtype\"])\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "caf565a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Energy: -3.55489866559794 Hartree.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Timings\n",
      "-------\n",
      "\n",
      "\u001b[1mObjective                Time (s)        % Total\u001b[0m\n",
      "------------------------------------------------\n",
      "\u001b[1mClassicals                  0.005           3.20\u001b[0m\n",
      " - DispersionD3        \u001b[37m     0.005          83.95\u001b[0m\n",
      " - Halogen             \u001b[37m     0.000           3.49\u001b[0m\n",
      " - Repulsion           \u001b[37m     0.001          11.72\u001b[0m\n",
      "\u001b[1mIntegrals                   0.004           2.43\u001b[0m\n",
      " - Overlap             \u001b[37m     0.002          56.72\u001b[0m\n",
      " - Core Hamiltonian    \u001b[37m     0.002          43.14\u001b[0m\n",
      "\u001b[1mSCF                         0.102          60.97\u001b[0m\n",
      " - Interaction Cache   \u001b[37m     0.001           0.72\u001b[0m\n",
      " - Potential           \u001b[37m     0.009           9.04\u001b[0m\n",
      " - Fock build          \u001b[37m     0.001           0.66\u001b[0m\n",
      " - Diagonalize         \u001b[37m     0.065          63.21\u001b[0m\n",
      " - Density             \u001b[37m     0.003           2.65\u001b[0m\n",
      " - Charges             \u001b[37m     0.003           2.88\u001b[0m\n",
      "\u001b[1mForces autograd             0.054          32.18\u001b[0m\n",
      "------------------------------------------------\n",
      "Sum                    \u001b[37m     0.166          98.78\u001b[0m\n",
      "\u001b[1mTotal                       0.168         100.00\u001b[0m\n",
      "Total Energy: -3.55489866559794 Hartree.\n",
      "Total Energy: -3.55489866559794 Hartree.\n"
     ]
    }
   ],
   "source": [
    "opts = {\"scf_mode\": \"full\", \"batch_mode\": 0, \"int_driver\": \"libcint\", \"maxiter\":10000}\n",
    "\n",
    "calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts, timer=True)\n",
    "calc.opts.cache = ConfigCache(enabled=False, density=True, fock=True, overlap=False)\n",
    "dxtb.timer.reset()\n",
    "e = calc.get_energy(positions, chrg=charges)\n",
    "dxtb.timer.start(\"Forces autograd\")\n",
    "forces = torch.autograd.grad(e, positions, retain_graph=True)[0]\n",
    "dxtb.timer.stop(\"Forces autograd\")\n",
    "dxtb.timer.print(v=0)\n",
    "\n",
    "results[f\"e_{opts['scf_mode']}\"] = e\n",
    "results[f\"forces_{opts['scf_mode']}\"] = forces\n",
    "results[f\"Fgrad_{opts['scf_mode']}\"] = torch.autograd.grad(calc.cache[\"fock\"].sum(), positions, retain_graph=True)[0]\n",
    "results[f\"Pgrad_{opts['scf_mode']}\"] = torch.autograd.grad(calc.get_density(positions, chrg=charges).sum(), positions, retain_graph=True)[0]\n",
    "\n",
    "# For reconnect modes\n",
    "scf_charges = calc.get_charges(positions, chrg=charges)\n",
    "scf_charge_mode = opts[\"scf_mode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74a880f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Energy: -3.55489865388107 Hartree.\n",
      "\n",
      "\n",
      "Timings\n",
      "-------\n",
      "\n",
      "\u001b[1mObjective                Time (s)        % Total\u001b[0m\n",
      "------------------------------------------------\n",
      "\u001b[1mClassicals                  0.004           4.17\u001b[0m\n",
      " - Halogen             \u001b[37m     0.000           3.76\u001b[0m\n",
      " - Repulsion           \u001b[37m     0.001          14.76\u001b[0m\n",
      " - DispersionD3        \u001b[37m     0.003          80.50\u001b[0m\n",
      "\u001b[1mIntegrals                   0.004           3.93\u001b[0m\n",
      " - Overlap             \u001b[37m     0.002          61.51\u001b[0m\n",
      " - Core Hamiltonian    \u001b[37m     0.002          38.33\u001b[0m\n",
      "\u001b[1mSCF                         0.086          83.81\u001b[0m\n",
      " - Interaction Cache   \u001b[37m     0.001           0.86\u001b[0m\n",
      " - Potential           \u001b[37m     0.069          79.56\u001b[0m\n",
      " - Fock build          \u001b[37m     0.001           0.58\u001b[0m\n",
      " - Diagonalize         \u001b[37m     0.054          62.39\u001b[0m\n",
      " - Density             \u001b[37m     0.002           2.68\u001b[0m\n",
      " - Charges             \u001b[37m     0.002           2.76\u001b[0m\n",
      "\u001b[1mForces autograd             0.007           6.87\u001b[0m\n",
      "------------------------------------------------\n",
      "Sum                    \u001b[37m     0.102          98.78\u001b[0m\n",
      "\u001b[1mTotal                       0.103         100.00\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Energy: -3.55489865388107 Hartree.\n",
      "Total Energy: -3.55489865388107 Hartree.\n"
     ]
    }
   ],
   "source": [
    "opts = {\"scf_mode\": \"implicit\", \"batch_mode\":0, \"int_driver\": \"libcint\", \"maxiter\":10000}\n",
    "\n",
    "calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts, timer=True)\n",
    "calc.opts.cache = ConfigCache(enabled=False, density=True, fock=True, overlap=False)\n",
    "dxtb.timer.reset()\n",
    "e = calc.get_energy(positions, chrg=charges)\n",
    "dxtb.timer.start(\"Forces autograd\")\n",
    "forces = torch.autograd.grad(e, positions, retain_graph=True)[0]\n",
    "dxtb.timer.stop(\"Forces autograd\")\n",
    "dxtb.timer.print(v=0)\n",
    "\n",
    "results[f\"e_{opts['scf_mode']}\"] = e\n",
    "results[f\"forces_{opts['scf_mode']}\"] = forces\n",
    "results[f\"Fgrad_{opts['scf_mode']}\"] = torch.autograd.grad(calc.cache[\"fock\"].sum(), positions, retain_graph=True)[0]\n",
    "results[f\"Pgrad_{opts['scf_mode']}\"] = torch.autograd.grad(calc.get_density(positions, chrg=charges).sum(), positions, retain_graph=True)[0]\n",
    "\n",
    "# For reconnect modes\n",
    "scf_charges = calc.get_charges(positions, chrg=charges)\n",
    "scf_charge_mode = opts[\"scf_mode\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7a5b6",
   "metadata": {},
   "source": [
    "# Gradchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41fc3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tad_mctc.autograd import dgradcheck\n",
    "\n",
    "from dxtb import GFN1_XTB as par\n",
    "from dxtb import Calculator, OutputHandler, labels\n",
    "from dxtb._src.typing import DD, Callable, Tensor\n",
    "from dxtb.config import ConfigCache\n",
    "\n",
    "# from .samples import samples\n",
    "\n",
    "tol = 1e-6\n",
    "\n",
    "def gradchecker(\n",
    "    dtype: torch.dtype, scp_mode: str = \"potential\", scf_mode: str = \"implicit\"\n",
    ") -> tuple[Callable[[Tensor], Tensor], Tensor]:\n",
    "    \"\"\"Prepare gradient check from `torch.autograd`.\"\"\"\n",
    "    dd: DD = {\"dtype\": dtype, \"device\": torch.device(\"cpu\")}\n",
    "\n",
    "    numbers = torch.tensor(atomic_numbers, device=dd[\"device\"], dtype=torch.int32)\n",
    "    positions = torch.tensor(coordinates, **dd)\n",
    "\n",
    "    opts = {\n",
    "        \"scf_mode\": scf_mode,\n",
    "        \"scp_mode\": scp_mode,\n",
    "    }\n",
    "\n",
    "    calc = Calculator(numbers, par, **dd, opts=opts)\n",
    "    calc.opts.cache = ConfigCache(enabled=False, fock=True)\n",
    "    OutputHandler.verbosity = 0\n",
    "\n",
    "    # variables to be differentiated\n",
    "    pos = positions.clone().requires_grad_(True)\n",
    "\n",
    "    def func(p: Tensor) -> Tensor:\n",
    "        _ = calc.get_energy(p)  # triggers Fock matrix computation\n",
    "        return calc.cache[\"fock\"]\n",
    "\n",
    "    return func, pos\n",
    "\n",
    "def test_grad_fock(dtype: torch.dtype, scp_mode: str, scf_mode: str) -> None:\n",
    "    \"\"\"\n",
    "    Check analytical gradient of Fock matrix against numerical\n",
    "    gradient from `torch.autograd.gradcheck`.\n",
    "    \"\"\"\n",
    "    func, diffvars = gradchecker(dtype, scp_mode, scf_mode)\n",
    "    assert dgradcheck(func, diffvars, atol=tol, fast_mode=True)\n",
    "\n",
    "test_grad_fock(torch.float64, \"potential\", \"full\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3dd037",
   "metadata": {},
   "source": [
    "# Manual jacobian calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46f91cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104709/3971275042.py:15: FutureWarning: `get_analytical_jacobian` was part of PyTorch's private API and not meant to be exposed. We are deprecating it and it will be removed in a future version of PyTorch. If you have a specific use for this or feature request for this to be a stable API, please file us an issue at https://github.com/pytorch/pytorch/issues/new\n",
      "  (J_flat,), reentrant, sizes_ok, types_ok = get_analytical_jacobian(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([87, 5776])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104709/3971275042.py:32: FutureWarning: `get_numerical_jacobian` was part of PyTorch's private API and not meant to be exposed. We are deprecating it and it will be removed in a future version of PyTorch. If you have a specific use for this or feature request for this to be a stable API, please file us an issue at https://github.com/pytorch/pytorch/issues/new\n",
      "  (J_flat,) = get_numerical_jacobian(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([87, 5776])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104709/3971275042.py:15: FutureWarning: `get_analytical_jacobian` was part of PyTorch's private API and not meant to be exposed. We are deprecating it and it will be removed in a future version of PyTorch. If you have a specific use for this or feature request for this to be a stable API, please file us an issue at https://github.com/pytorch/pytorch/issues/new\n",
      "  (J_flat,), reentrant, sizes_ok, types_ok = get_analytical_jacobian(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([87, 5776])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104709/3971275042.py:32: FutureWarning: `get_numerical_jacobian` was part of PyTorch's private API and not meant to be exposed. We are deprecating it and it will be removed in a future version of PyTorch. If you have a specific use for this or feature request for this to be a stable API, please file us an issue at https://github.com/pytorch/pytorch/issues/new\n",
      "  (J_flat,) = get_numerical_jacobian(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([87, 5776])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.gradcheck import (\n",
    "    get_analytical_jacobian,\n",
    "    get_numerical_jacobian,\n",
    ")\n",
    "\n",
    "# — your existing gradchecker() definition must be in scope here —\n",
    "\n",
    "def analytical_jacobian(fn, inputs):\n",
    "    # wrap input in a 1‑tuple, flatten output to 1‑D\n",
    "    inputs_tup = (inputs,)\n",
    "    y = fn(inputs)            # e.g. your Fock matrix, shape [M,N]\n",
    "    y_flat = y.reshape(-1)    # shape [M*N]\n",
    "    # returns a tuple of Jacobians—one per input\n",
    "    (J_flat,), reentrant, sizes_ok, types_ok = get_analytical_jacobian(\n",
    "        inputs_tup,\n",
    "        y_flat,\n",
    "        nondet_tol=0.0,\n",
    "        grad_out=1.0,\n",
    "    )\n",
    "    # J_flat shape: (inputs.numel(), y_flat.numel())\n",
    "    return J_flat\n",
    "\n",
    "def numerical_jacobian(fn, inputs, eps=1e-6):\n",
    "    # we need a function that takes a tuple of inputs\n",
    "    # and returns a flat (1‑D) output\n",
    "    def flat_fn(inp_tuple):\n",
    "        x = inp_tuple[0]\n",
    "        y = fn(x)\n",
    "        return y.reshape(-1)\n",
    "    # get_numerical_jacobian returns one Jacobian per input\n",
    "    (J_flat,) = get_numerical_jacobian(\n",
    "        flat_fn,\n",
    "        inputs,   # single Tensor; internals call _as_tuple on it\n",
    "        eps=eps,\n",
    "    )\n",
    "    # J_flat shape: (inputs.numel(), y_flat.numel())\n",
    "    return J_flat\n",
    "\n",
    "# —— usage example —— \n",
    "\n",
    "# build your function + variables\n",
    "SCF_MODE = \"implicit\"\n",
    "SCP_MODE = \"potential\"\n",
    "func32, pos32 = gradchecker(torch.float32, SCP_MODE, SCF_MODE)\n",
    "func64, pos64 = gradchecker(torch.float64, SCP_MODE, SCF_MODE)\n",
    "\n",
    "# compute slow/full analytical and numerical\n",
    "J_an32 = analytical_jacobian(func32, pos32); print(J_an32.shape)\n",
    "J_num32 = numerical_jacobian(func32, pos32); print(J_num32.shape)\n",
    "J_an64 = analytical_jacobian(func64, pos64); print(J_an64.shape)\n",
    "J_num64 = numerical_jacobian(func64, pos64); print(J_num64.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c93b9a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCF_MODE: implicit, SCP_MODE: potential\n",
      "Number of carbon atoms in alkane_9_carbons: 9\n",
      "Nb of atoms: 29\n",
      "J_an32: torch.float32\n",
      "J_num32: torch.float32\n",
      "J_an64: torch.float64\n",
      "J_num64: torch.float64\n",
      "\n",
      "max_diff_32an_64an: 5.18e-07\n",
      "max_diff_32nu_64nu: 2.47e+01\n",
      "\n",
      "max_diff_32an_32nu: 2.47e+01\n",
      "max_diff_64an_64nu: 1.15e-01\n",
      "max_diff_32an_64nu: 1.15e-01\n",
      "\n",
      "max_rdiff_32an_64an: 1.02e+14\n",
      "max_rdiff_32nu_64nu: 3.70e+14\n",
      "\n",
      "max_rdiff_32an_32nu: 1.02e+14\n",
      "max_rdiff_64an_64nu: 7.93e+12\n",
      "max_rdiff_32an_64nu: 7.93e+12\n"
     ]
    }
   ],
   "source": [
    "# Print datatypes\n",
    "print(f\"SCF_MODE: {SCF_MODE}, SCP_MODE: {SCP_MODE}\")\n",
    "print(f\"Number of carbon atoms in {mol_name}: {N_Cs}\")\n",
    "print(f\"Nb of atoms: {len(atomic_numbers)}\")\n",
    "print(f\"J_an32: {J_an32.dtype}\")\n",
    "print(f\"J_num32: {J_num32.dtype}\")\n",
    "print(f\"J_an64: {J_an64.dtype}\")\n",
    "print(f\"J_num64: {J_num64.dtype}\")\n",
    "print()\n",
    "\n",
    "assert J_an32.shape == J_num32.shape\n",
    "max_diff_32an_64an = torch.max(torch.abs(J_an32 - J_an64))\n",
    "max_diff_32nu_64nu = torch.max(torch.abs(J_num32 - J_num64))\n",
    "\n",
    "max_diff_32an_32nu = torch.max(torch.abs(J_an32 - J_num32))\n",
    "max_diff_64an_64nu = torch.max(torch.abs(J_an64 - J_num64))\n",
    "max_diff_32an_64nu = torch.max(torch.abs(J_an32 - J_num64))\n",
    "\n",
    "print(f\"max_diff_32an_64an: {max_diff_32an_64an:.2e}\")\n",
    "print(f\"max_diff_32nu_64nu: {max_diff_32nu_64nu:.2e}\\n\")\n",
    "print(f\"max_diff_32an_32nu: {max_diff_32an_32nu:.2e}\")\n",
    "print(f\"max_diff_64an_64nu: {max_diff_64an_64nu:.2e}\")\n",
    "print(f\"max_diff_32an_64nu: {max_diff_32an_64nu:.2e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check the relative max difference \n",
    "max_rdiff_32an_64an = torch.max(torch.abs(J_an32 - J_an64) / (torch.abs(J_an32) + 1e-15))\n",
    "max_rdiff_32nu_64nu = torch.max(torch.abs(J_num32 - J_num64) / (torch.abs(J_num32) + 1e-15))\n",
    "\n",
    "max_rdiff_32an_32nu = torch.max(torch.abs(J_an32 - J_num32) / (torch.abs(J_an32) + 1e-15))\n",
    "max_rdiff_64an_64nu = torch.max(torch.abs(J_an64 - J_num64) / (torch.abs(J_an64) + 1e-15))\n",
    "max_rdiff_32an_64nu = torch.max(torch.abs(J_an32 - J_num64) / (torch.abs(J_an32) + 1e-15))\n",
    "print(f\"max_rdiff_32an_64an: {max_rdiff_32an_32nu:.2e}\")\n",
    "print(f\"max_rdiff_32nu_64nu: {max_rdiff_32nu_64nu:.2e}\\n\")\n",
    "print(f\"max_rdiff_32an_32nu: {max_rdiff_32an_32nu:.2e}\")\n",
    "print(f\"max_rdiff_64an_64nu: {max_rdiff_64an_64nu:.2e}\")\n",
    "print(f\"max_rdiff_32an_64nu: {max_rdiff_32an_64nu:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a7afa",
   "metadata": {},
   "source": [
    "# Other attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import gradcheck\n",
    "from dxtb import OutputHandler\n",
    "from dxtb.config import ConfigCache\n",
    "\n",
    "OutputHandler.verbosity = 0\n",
    "\n",
    "# Inputs (must be float64)\n",
    "positions_d = positions.detach().double().requires_grad_()\n",
    "charges_d = charges.double()\n",
    "\n",
    "calc.opts.cache = ConfigCache(enabled=False, fock=True, density=True)\n",
    "\n",
    "def run_gradcheck(fn, inputs):\n",
    "    # gradcheck assumes float64 and requires_grad=True\n",
    "    inputs = tuple(i.detach().double().requires_grad_() for i in inputs)\n",
    "    passed = gradcheck(fn, inputs, eps=1e-6, atol=1e-3, rtol=1e-3, nondet_tol=1e-5)\n",
    "    print(f\"Gradcheck passed: {passed}\")\n",
    "\n",
    "# Energy (scalar output)\n",
    "def energy_fn(pos):\n",
    "    return calc.get_energy(pos, chrg=charges_d)\n",
    "\n",
    "print(\"Energy\")\n",
    "run_gradcheck(energy_fn, (positions_d,))\n",
    "\n",
    "# Fock (matrix output)\n",
    "def fock_fn(pos):\n",
    "    _ = calc.get_energy(pos, chrg=charges_d)  # populate cache\n",
    "    return calc.cache[\"fock\"]\n",
    "\n",
    "print(\"Fock\")\n",
    "run_gradcheck(fock_fn, (positions_d,))\n",
    "\n",
    "# Density (matrix output)\n",
    "def density_fn(pos):\n",
    "    return calc.get_density(pos, chrg=charges_d)\n",
    "\n",
    "print(\"Density\")\n",
    "run_gradcheck(density_fn, (positions_d,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a25de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import gradcheck\n",
    "from dxtb import OutputHandler\n",
    "\n",
    "OutputHandler.verbosity = 0\n",
    "\n",
    "def run_gradcheck(fn, inputs):\n",
    "    inputs = tuple(i.detach().requires_grad_() for i in inputs)\n",
    "    passed = gradcheck(fn, inputs)\n",
    "    print(f\"Gradcheck passed: {passed}\")\n",
    "\n",
    "# Functions must return tuple of tensor outputs in float64\n",
    "def energy_fn(pos):\n",
    "    return (calc.get_energy(pos, chrg=charges),)\n",
    "\n",
    "def fock_fn(pos):\n",
    "    calc.get_energy(pos, chrg=charges)  # populate cache\n",
    "    return (calc.cache[\"fock\"].sum(),)\n",
    "\n",
    "def density_fn(pos):\n",
    "    return (calc.get_density(pos, chrg=charges).sum(),)\n",
    "\n",
    "# Inputs\n",
    "positions_d = positions.detach().requires_grad_()\n",
    "charges_d = charges\n",
    "\n",
    "print(\"Energy\")\n",
    "run_gradcheck(energy_fn, (positions_d,))\n",
    "\n",
    "print(\"Fock\")\n",
    "run_gradcheck(fock_fn, (positions_d,))\n",
    "\n",
    "print(\"Density\")\n",
    "run_gradcheck(density_fn, (positions_d,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd07ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import gradcheck\n",
    "from torch.autograd.gradcheck import _get_numerical_jacobian, _as_tuple\n",
    "from dxtb import OutputHandler\n",
    "\n",
    "OutputHandler.verbosity = 0\n",
    "\n",
    "def compare_grads(fn, inputs):\n",
    "    # Prepare input\n",
    "    inputs = tuple(i.detach().requires_grad_() for i in _as_tuple(inputs))\n",
    "    output = fn(*inputs)\n",
    "    output = _as_tuple(output)\n",
    "\n",
    "    # Compute autograd\n",
    "    autograd_grads = torch.autograd.grad(output, inputs, grad_outputs=[torch.ones_like(o) for o in output], retain_graph=True)\n",
    "\n",
    "    # Compute numerical\n",
    "    numerical_grads = _get_numerical_jacobian(fn, inputs, eps=1e-6)\n",
    "\n",
    "    # Print comparison\n",
    "    for i, (a, n) in enumerate(zip(autograd_grads, numerical_grads)):\n",
    "        n_tensor = n[0][0]  # FIXED: Unwrap twice\n",
    "        print(f\"[Input {i}] max(abs diff): {(a - n_tensor).abs().max().item():.2e}\")\n",
    "        # print(f\"Autograd:\\n{a}\\nNumerical:\\n{n_tensor}\")\n",
    "\n",
    "def energy_fn(pos):\n",
    "    return calc.get_energy(pos, chrg=charges)\n",
    "print(\"Energy\")\n",
    "compare_grads(energy_fn, (positions,))\n",
    "\n",
    "def fock_fn(pos):\n",
    "    calc.get_energy(pos, chrg=charges)\n",
    "    return calc.cache[\"fock\"].sum()\n",
    "print(\"Fock\")\n",
    "compare_grads(fock_fn, (positions,))\n",
    "\n",
    "def density_fn(pos):\n",
    "    return calc.get_density(pos, chrg=charges).sum()\n",
    "print(\"Density\")\n",
    "compare_grads(density_fn, (positions,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6cb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dxtb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
