{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b4dda4",
   "metadata": {},
   "source": [
    "# You can run the cell below multiple times and see that randomly 1 out of 2 errors appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d30034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dxtb\n",
    "\n",
    "problematic_batch_path = \"problematic_batch_for_full.pt\"\n",
    "\n",
    "dd = {\"dtype\": torch.float64, \"device\": torch.device(\"cuda:0\")}\n",
    "opts = {\"scf_mode\": \"full\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "# Load the problematic batch\n",
    "problematic_batch = torch.load(problematic_batch_path, weights_only=False)\n",
    "numbers = problematic_batch[\"numbers\"].to(dd[\"device\"])\n",
    "positions = problematic_batch[\"positions\"].to(**dd)\n",
    "\n",
    "batch_size = numbers.shape[0]\n",
    "charges = torch.full((batch_size,), 0, **dd)\n",
    "\n",
    "calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "e = calc.get_energy(positions, chrg=charges)\n",
    "forces = torch.autograd.grad(sum(e), positions, retain_graph=True)[0]\n",
    "\n",
    "# Features calc\n",
    "func = lambda e, p: -torch.autograd.grad(e.sum(), p, retain_graph=True)[0]\n",
    "res = func(e, positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d36db4",
   "metadata": {},
   "source": [
    "# Or just run this looped version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2743f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:47,  2.09it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "could not compute gradients for some functions",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Features calc\u001b[39;00m\n\u001b[32m     23\u001b[39m func = \u001b[38;5;28;01mlambda\u001b[39;00m e, p: -torch.autograd.grad(e.sum(), p, retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(e, p)\u001b[39m\n\u001b[32m     20\u001b[39m forces = torch.autograd.grad(\u001b[38;5;28msum\u001b[39m(e), positions, retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Features calc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m func = \u001b[38;5;28;01mlambda\u001b[39;00m e, p: -\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     24\u001b[39m res = func(e, positions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dxtb_dev/lib/python3.11/site-packages/torch/autograd/__init__.py:502\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    498\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    499\u001b[39m         grad_outputs_\n\u001b[32m    500\u001b[39m     )\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    513\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    514\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    515\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dxtb_dev/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: could not compute gradients for some functions"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dxtb\n",
    "from tqdm import tqdm\n",
    "\n",
    "dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "opts = {\"scf_mode\": \"full\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "# Load the problematic batch\n",
    "problematic_batch = torch.load(problematic_batch_path, weights_only=False)\n",
    "numbers = problematic_batch[\"numbers\"].to(dd[\"device\"])\n",
    "positions = problematic_batch[\"positions\"].to(**dd)\n",
    "\n",
    "batch_size = numbers.shape[0]\n",
    "charges = torch.full((batch_size,), 0, **dd)\n",
    "\n",
    "calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    e = calc.get_energy(positions, chrg=charges)\n",
    "    forces = torch.autograd.grad(sum(e), positions, retain_graph=True)[0]\n",
    "\n",
    "    # Features calc\n",
    "    func = lambda e, p: -torch.autograd.grad(e.sum(), p, retain_graph=True)[0]\n",
    "    res = func(e, positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7674a8",
   "metadata": {},
   "source": [
    "# Checks about the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c49088",
   "metadata": {},
   "source": [
    "It is a sample from the Transition1x dataset. Specifically C2H2N2O/rxn2091. So a single molecule at different steps in the reaction path. Units of the positions are Bohr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8933865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers.shape: torch.Size([64, 7])\n",
      "positions.shape: torch.Size([64, 7, 3])\n",
      "\n",
      "Subset:\n",
      "numbers[0]: tensor([8, 6, 7, 6, 7, 1, 1], device='cuda:0', dtype=torch.int32)\n",
      "positions[0]: tensor([[ 0.8494,  3.3460,  0.0348],\n",
      "        [ 1.2790, -1.3759, -0.2903],\n",
      "        [-0.2177, -1.3857, -2.1151],\n",
      "        [-1.1522, -0.2871,  0.3423],\n",
      "        [-1.1985,  2.4734,  0.5247],\n",
      "        [ 3.2372, -1.7409,  0.1731],\n",
      "        [-2.7249, -1.2143,  1.2866]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Properties of the problematic batch:\n",
      "isnan positions: False\n",
      "isinf positions: False\n",
      "max positions: 3.877077102661133\n",
      "min positions: -3.9229071140289307\n"
     ]
    }
   ],
   "source": [
    "problematic_batch = torch.load(problematic_batch_path, weights_only=False)\n",
    "numbers = problematic_batch[\"numbers\"]\n",
    "positions = problematic_batch[\"positions\"]\n",
    "\n",
    "print(f\"numbers.shape: {numbers.shape}\")\n",
    "print(f\"positions.shape: {positions.shape}\")\n",
    "\n",
    "print(f\"\\nSubset:\")\n",
    "idx = 0\n",
    "print(f\"numbers[0]: {numbers[idx]}\")\n",
    "print(f\"positions[0]: {positions[idx]}\")\n",
    "\n",
    "print(\"\\nProperties of the problematic batch:\")\n",
    "print(f\"isnan positions: {torch.isnan(positions).any()}\")\n",
    "print(f\"isinf positions: {torch.isinf(positions).any()}\")\n",
    "print(f\"max positions: {positions.max()}\")\n",
    "print(f\"min positions: {positions.min()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ceef5",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f27ac10",
   "metadata": {},
   "source": [
    "- Env:\n",
    "    - Python 3.11\n",
    "    - Just pip install -e . of the main branch of dxtb\n",
    "    - pip install jupyter notebook tqdm torch tad-libcint\n",
    "\n",
    "    - Torch 2.5.1, CUDA 12.4, driver 550.120\n",
    "\n",
    "\n",
    "- If restarting the kernel after each run it does not break. Maybe something is cached in the calculator? \n",
    "\n",
    "- It is only breaking when using a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23cd5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dxtb_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
