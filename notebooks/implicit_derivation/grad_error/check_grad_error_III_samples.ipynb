{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175aa5cb",
   "metadata": {},
   "source": [
    "# Function in notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dxtb\n",
    "\n",
    "def generate_xtb_features_dxtb(\n",
    "        element_numbers,\n",
    "        coordinates,\n",
    "        charge=0,\n",
    "        spin=0,\n",
    "        ):\n",
    "    \n",
    "\n",
    "    dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "    opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "    calc = dxtb.Calculator(element_numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "    energy = calc.get_energy(coordinates, chrg=charge, spin=spin)\n",
    "    forces = -torch.autograd.grad(energy.sum(), coordinates, retain_graph=True)[0]\n",
    "\n",
    "    return energy, forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb81d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 0 datapoints [00:00, ? datapoints/s]/tmp/ipykernel_403739/3155021629.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647429097/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  z_batch = torch.tensor([zs] * len(pos_batch), device=dd[\"device\"])  # [B, N]\n",
      "C2H2N2O/rxn2091: : 2 datapoints [00:01,  1.97 datapoints/s, total=192]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "_Map_base::at",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     72\u001b[39m forces = torch.autograd.grad(\u001b[38;5;28msum\u001b[39m(e), batch[\u001b[33m'\u001b[39m\u001b[33mpos\u001b[39m\u001b[33m'\u001b[39m], retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Features calc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m res = \u001b[43mgenerate_xtb_features_dxtb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpos\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcharge\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcharges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspin\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mgenerate_xtb_features_dxtb\u001b[39m\u001b[34m(element_numbers, coordinates, charge, spin)\u001b[39m\n\u001b[32m     22\u001b[39m calc = dxtb.Calculator(element_numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n\u001b[32m     24\u001b[39m energy = calc.get_energy(coordinates, chrg=charge, spin=spin)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m forces = -\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43menergy\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m energy, forces\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/orbnet-ete/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    492\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    493\u001b[39m         grad_outputs_\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    507\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    508\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    509\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/orbnet-ete/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: _Map_base::at"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "\n",
    "class TransitionBatchDataset(IterableDataset):\n",
    "    def __init__(self, hdf5_path, split=\"val\", batch_size=64, mol_names=None):\n",
    "        super().__init__()\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        self.mol_names = mol_names\n",
    "\n",
    "    def __iter__(self):\n",
    "        with h5py.File(self.hdf5_path, \"r\") as f:\n",
    "            for mol_name in list(self.mol_names or f[f\"{self.split}\"].keys()):\n",
    "                mol_group = f[f\"{self.split}/{mol_name}\"]\n",
    "                for rxn_name in mol_group.keys():\n",
    "                    rxn_group = mol_group[rxn_name]\n",
    "                    positions = rxn_group[\"positions\"]\n",
    "                    zs = rxn_group[\"atomic_numbers\"][()]\n",
    "                    n_samples = len(positions)\n",
    "\n",
    "                    for i in range(0, n_samples, self.batch_size):\n",
    "                        pos_batch = torch.tensor(positions[i:i+self.batch_size], **dd) / 0.529177 # [A] -> [Bohr]\n",
    "                        pos_batch.requires_grad_(True)\n",
    "                        z_batch = torch.tensor([zs] * len(pos_batch), device=dd[\"device\"])  # [B, N]\n",
    "                        yield {\n",
    "                            \"mol_name\": mol_name,\n",
    "                            \"rxn_name\": rxn_name,\n",
    "                            \"z\": z_batch,\n",
    "                            \"pos\": pos_batch,\n",
    "                            \"batch_size\": len(pos_batch)\n",
    "                        }\n",
    "\n",
    "# Create dataset + dataloader\n",
    "dataset = TransitionBatchDataset(\n",
    "    hdf5_path=\"../../../../../data/Transition1x/data/transition1x.h5\",\n",
    "    batch_size=64,\n",
    "    mol_names=None\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=None)\n",
    "\n",
    "# Wrap in tqdm and track sample count\n",
    "sample_count = 0\n",
    "pbar = tqdm(dataloader, desc=\"Processing\", unit=\" datapoints\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for batch in pbar:\n",
    "    i += 1\n",
    "    # if i <53:\n",
    "    #     continue\n",
    "    sample_count += batch[\"batch_size\"]\n",
    "    pbar.set_description(f\"{batch['mol_name']}/{batch['rxn_name']}\")\n",
    "    pbar.set_postfix(total=sample_count)\n",
    "    \n",
    "    # print(f\"z {batch['z']}\")\n",
    "    # print(f\"pos {batch['pos']}\")\n",
    "\n",
    "    numbers = batch[\"z\"]\n",
    "    positions = batch[\"pos\"]\n",
    "\n",
    "    # DXTB CALC\n",
    "    dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "    opts = {\"scf_mode\": \"full\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "    batch_size = batch['z'].shape[0]\n",
    "    charges = torch.full((batch_size,), 0, **dd)\n",
    "    spin = torch.full((batch_size,), 0, **dd)\n",
    "\n",
    "    calc = dxtb.Calculator(batch['z'], dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "    e = calc.get_energy(batch['pos'], chrg=charges, spin=spin)\n",
    "    forces = torch.autograd.grad(sum(e), batch['pos'], retain_graph=True)[0]\n",
    "    \n",
    "    # Features calc\n",
    "    res = generate_xtb_features_dxtb(\n",
    "        batch[\"z\"],\n",
    "        batch[\"pos\"],\n",
    "        charge=charges,\n",
    "        spin=spin\n",
    "    )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90995609",
   "metadata": {},
   "source": [
    "# Saving problematic batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea8bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the numbers and positions\n",
    "torch.save(\n",
    "    {\n",
    "        \"numbers\": numbers,\n",
    "        \"positions\": positions,\n",
    "    },\n",
    "    \"problematic_batch.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8caf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the problematic batch\n",
    "problematic_batch = torch.load(\"problematic_batch.pt\", weights_only=False)\n",
    "numbers = problematic_batch[\"numbers\"]\n",
    "positions = problematic_batch[\"positions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe4fe37",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "_Map_base::at",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m forces = torch.autograd.grad(\u001b[38;5;28msum\u001b[39m(e), positions, retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Features calc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m res = \u001b[43mgenerate_xtb_features_dxtb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m   \u001b[49m\u001b[43mnumbers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcharge\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcharges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mgenerate_xtb_features_dxtb\u001b[39m\u001b[34m(element_numbers, coordinates, charge, spin)\u001b[39m\n\u001b[32m     21\u001b[39m calc = dxtb.Calculator(element_numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n\u001b[32m     23\u001b[39m energy = calc.get_energy(coordinates, chrg=charge, spin=spin)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m forces = -\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43menergy\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m energy, forces\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/orbnet-ete/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    492\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    493\u001b[39m         grad_outputs_\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    507\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    508\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    509\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/orbnet-ete/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: _Map_base::at"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dxtb\n",
    "\n",
    "def generate_xtb_features_dxtb(\n",
    "        element_numbers,\n",
    "        coordinates,\n",
    "        charge=0,\n",
    "        spin=0,\n",
    "        ):\n",
    "    \n",
    "\n",
    "    dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "    opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "    calc = dxtb.Calculator(element_numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "    energy = calc.get_energy(coordinates, chrg=charge, spin=spin)\n",
    "    forces = -torch.autograd.grad(energy.sum(), coordinates, retain_graph=True)[0]\n",
    "\n",
    "    return energy, forces\n",
    "\n",
    "\n",
    "# Load the problematic batch\n",
    "problematic_batch = torch.load(\"problematic_batch.pt\", weights_only=False)\n",
    "numbers = problematic_batch[\"numbers\"]\n",
    "positions = problematic_batch[\"positions\"]\n",
    "\n",
    "dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "opts = {\"scf_mode\": \"full\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "batch_size = numbers.shape[0]\n",
    "charges = torch.full((batch_size,), 0, **dd)\n",
    "spin = torch.full((batch_size,), 0, **dd)\n",
    "\n",
    "calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "e = calc.get_energy(positions, chrg=charges, spin=spin)\n",
    "forces = torch.autograd.grad(sum(e), positions, retain_graph=True)[0]\n",
    "\n",
    "# Features calc\n",
    "res = generate_xtb_features_dxtb(\n",
    "   numbers,\n",
    "    positions,\n",
    "    charge=charges,\n",
    "    spin=spin,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d2198",
   "metadata": {},
   "source": [
    "# Disable caching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315f8e92",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "_Map_base::at",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m forces = torch.autograd.grad(\u001b[38;5;28msum\u001b[39m(e), positions, retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Features calc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m res = \u001b[43mgenerate_xtb_features_dxtb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m   \u001b[49m\u001b[43mnumbers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcharge\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcharges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mgenerate_xtb_features_dxtb\u001b[39m\u001b[34m(element_numbers, coordinates, charge, spin)\u001b[39m\n\u001b[32m     22\u001b[39m calc = dxtb.Calculator(element_numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n\u001b[32m     24\u001b[39m energy = calc.get_energy(coordinates, chrg=charge, spin=spin)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m forces = -\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43menergy\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m energy, forces\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/orbnet-ete/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    492\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    493\u001b[39m         grad_outputs_\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    507\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    508\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    509\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/orbnet-ete/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: _Map_base::at"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dxtb\n",
    "from dxtb.config import ConfigCache\n",
    "\n",
    "def generate_xtb_features_dxtb(\n",
    "        element_numbers,\n",
    "        coordinates,\n",
    "        charge=0,\n",
    "        spin=0,\n",
    "        ):\n",
    "    \n",
    "\n",
    "    dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "    opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "    calc = dxtb.Calculator(element_numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "    energy = calc.get_energy(coordinates, chrg=charge, spin=spin)\n",
    "    forces = -torch.autograd.grad(energy.sum(), coordinates, retain_graph=True)[0]\n",
    "\n",
    "    return energy, forces\n",
    "\n",
    "# Load the problematic batch\n",
    "problematic_batch = torch.load(\"problematic_batch.pt\", weights_only=False)\n",
    "numbers = problematic_batch[\"numbers\"]\n",
    "positions = problematic_batch[\"positions\"]\n",
    "\n",
    "dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "opts = {\"scf_mode\": \"full\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "batch_size = numbers.shape[0]\n",
    "charges = torch.full((batch_size,), 0, **dd)\n",
    "spin = torch.full((batch_size,), 0, **dd)\n",
    "\n",
    "calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "calc.opts.cache = ConfigCache(enabled=False, density=False, fock=False)\n",
    "\n",
    "e = calc.get_energy(positions, chrg=charges, spin=spin)\n",
    "forces = torch.autograd.grad(sum(e), positions, retain_graph=True)[0]\n",
    "\n",
    "# Features calc\n",
    "res = generate_xtb_features_dxtb(\n",
    "   numbers,\n",
    "    positions,\n",
    "    charge=charges,\n",
    "    spin=spin,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab17bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbnet-ete",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
