{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b4dda4",
   "metadata": {},
   "source": [
    "# You can run the cell below multiple times and see that randomly 1 out of 2 errors appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426af90a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "_Map_base::at",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m forces = torch.autograd.grad(\u001b[38;5;28msum\u001b[39m(e), positions, retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Features calc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m res = \u001b[43mgenerate_xtb_features_dxtb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m   \u001b[49m\u001b[43mnumbers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcharge\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcharges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mgenerate_xtb_features_dxtb\u001b[39m\u001b[34m(element_numbers, coordinates, charge, spin)\u001b[39m\n\u001b[32m     15\u001b[39m calc = dxtb.Calculator(element_numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n\u001b[32m     17\u001b[39m energy = calc.get_energy(coordinates, chrg=charge, spin=spin)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m forces = -\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43menergy\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m energy, forces\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dxtb_dev/lib/python3.11/site-packages/torch/autograd/__init__.py:502\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    498\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    499\u001b[39m         grad_outputs_\n\u001b[32m    500\u001b[39m     )\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    513\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    514\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    515\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dxtb_dev/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: _Map_base::at"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dxtb\n",
    "\n",
    "def generate_xtb_features_dxtb(\n",
    "        element_numbers,\n",
    "        coordinates,\n",
    "        charge=0,\n",
    "        spin=0,\n",
    "        ):\n",
    "    \n",
    "\n",
    "    dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "    opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "    calc = dxtb.Calculator(element_numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "    energy = calc.get_energy(coordinates, chrg=charge, spin=spin)\n",
    "    forces = -torch.autograd.grad(energy.sum(), coordinates, retain_graph=True)[0]\n",
    "\n",
    "    return energy, forces\n",
    "\n",
    "\n",
    "# Load the problematic batch\n",
    "dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "problematic_batch = torch.load(\"problematic_batch.pt\", weights_only=False)\n",
    "numbers = problematic_batch[\"numbers\"].to(dd[\"device\"])\n",
    "positions = problematic_batch[\"positions\"].to(**dd)\n",
    "\n",
    "opts = {\"scf_mode\": \"full\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "batch_size = numbers.shape[0]\n",
    "charges = torch.full((batch_size,), 0, **dd)\n",
    "spin = torch.full((batch_size,), 0, **dd)\n",
    "\n",
    "calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "e = calc.get_energy(positions, chrg=charges, spin=spin)\n",
    "forces = torch.autograd.grad(sum(e), positions, retain_graph=True)[0]\n",
    "\n",
    "# Features calc\n",
    "res = generate_xtb_features_dxtb(\n",
    "   numbers,\n",
    "    positions,\n",
    "    charge=charges,\n",
    "    spin=spin,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d36db4",
   "metadata": {},
   "source": [
    "# Or just run this looped version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65229f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:01<00:04,  1.66it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "could not compute gradients for some functions",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     36\u001b[39m spin = torch.full((batch_size,), \u001b[32m0\u001b[39m, **dd)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Features calc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m res = \u001b[43mgenerate_xtb_features_dxtb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m\u001b[49m\u001b[43mnumbers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcharge\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcharges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n\u001b[32m     49\u001b[39m e = calc.get_energy(positions, chrg=charges, spin=spin)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mgenerate_xtb_features_dxtb\u001b[39m\u001b[34m(element_numbers, coordinates, charge, spin)\u001b[39m\n\u001b[32m     17\u001b[39m calc = dxtb.Calculator(element_numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n\u001b[32m     19\u001b[39m energy = calc.get_energy(coordinates, chrg=charge, spin=spin)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m forces = -\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43menergy\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m energy, forces\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dxtb_dev/lib/python3.11/site-packages/torch/autograd/__init__.py:502\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    498\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    499\u001b[39m         grad_outputs_\n\u001b[32m    500\u001b[39m     )\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    513\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    514\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    515\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dxtb_dev/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: could not compute gradients for some functions"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dxtb\n",
    "\n",
    "def generate_xtb_features_dxtb(\n",
    "        element_numbers,\n",
    "        coordinates,\n",
    "        charge=0,\n",
    "        spin=0,\n",
    "        ):\n",
    "    \n",
    "\n",
    "    dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "    opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "    calc = dxtb.Calculator(element_numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "    energy = calc.get_energy(coordinates, chrg=charge, spin=spin)\n",
    "    forces = -torch.autograd.grad(energy.sum(), coordinates, retain_graph=True)[0]\n",
    "\n",
    "    return energy, forces\n",
    "\n",
    "\n",
    "# Load the problematic batch\n",
    "problematic_batch = torch.load(\"problematic_batch.pt\", weights_only=False)\n",
    "numbers = problematic_batch[\"numbers\"]\n",
    "positions = problematic_batch[\"positions\"]\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "    opts = {\"scf_mode\": \"full\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "    batch_size = numbers.shape[0]\n",
    "    charges = torch.full((batch_size,), 0, **dd)\n",
    "    spin = torch.full((batch_size,), 0, **dd)\n",
    "\n",
    "    calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "    e = calc.get_energy(positions, chrg=charges, spin=spin)\n",
    "    forces = torch.autograd.grad(sum(e), positions, retain_graph=True)[0]\n",
    "\n",
    "    # Features calc\n",
    "    res = generate_xtb_features_dxtb(\n",
    "    numbers,\n",
    "        positions,\n",
    "        charge=charges,\n",
    "        spin=spin,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7674a8",
   "metadata": {},
   "source": [
    "# Checks about the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c49088",
   "metadata": {},
   "source": [
    "It is a sample from the Transition1x dataset. Specifically C2H2N2O/rxn2091. So a single molecule at different steps in the reaction path. Units of the positions are Bohr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8933865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers.shape: torch.Size([64, 7])\n",
      "positions.shape: torch.Size([64, 7, 3])\n",
      "\n",
      "Subset:\n",
      "numbers[0]: tensor([8, 6, 7, 7, 6, 1, 1], device='cuda:0', dtype=torch.int32)\n",
      "positions[0]: tensor([[ 1.5977, -3.0446,  0.4738],\n",
      "        [ 1.9159, -0.8995, -0.2115],\n",
      "        [ 0.4210,  1.1152,  0.7972],\n",
      "        [-1.3898,  2.4456, -1.2272],\n",
      "        [-2.1598,  0.8804,  0.3519],\n",
      "        [ 3.4245, -0.2894, -1.5262],\n",
      "        [-3.7901, -0.1763,  0.9885]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Properties of the problematic batch:\n",
      "isnan positions: False\n",
      "isinf positions: False\n",
      "max positions: 3.943268299102783\n",
      "min positions: -3.950294017791748\n"
     ]
    }
   ],
   "source": [
    "problematic_batch = torch.load(\"problematic_batch.pt\", weights_only=False)\n",
    "numbers = problematic_batch[\"numbers\"]\n",
    "positions = problematic_batch[\"positions\"]\n",
    "\n",
    "print(f\"numbers.shape: {numbers.shape}\")\n",
    "print(f\"positions.shape: {positions.shape}\")\n",
    "\n",
    "print(f\"\\nSubset:\")\n",
    "idx = 0\n",
    "print(f\"numbers[0]: {numbers[idx]}\")\n",
    "print(f\"positions[0]: {positions[idx]}\")\n",
    "\n",
    "print(\"\\nProperties of the problematic batch:\")\n",
    "print(f\"isnan positions: {torch.isnan(positions).any()}\")\n",
    "print(f\"isinf positions: {torch.isinf(positions).any()}\")\n",
    "print(f\"max positions: {positions.max()}\")\n",
    "print(f\"min positions: {positions.min()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ceef5",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f27ac10",
   "metadata": {},
   "source": [
    "- Env:\n",
    "    - Python 3.11\n",
    "    - Just pip install -e . of the main branch of dxtb\n",
    "    - pip install jupyter notebook tqdm torch tad-libcint\n",
    "\n",
    "\n",
    "- If restarting the kernel after each run it does not break. Maybe something is cached in the calculator? \n",
    "\n",
    "- It is only breaking when using a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23cd5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dxtb_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
