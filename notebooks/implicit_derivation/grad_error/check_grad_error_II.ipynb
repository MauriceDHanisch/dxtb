{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5468b4a",
   "metadata": {},
   "source": [
    "# Function in Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70178afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Resolve the absolute path to the \"orbspin\" directory relative to the notebook's location\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "features_path = os.path.abspath(\n",
    "    os.path.join(notebook_dir, \"../../../../orbspin\")\n",
    ")\n",
    "util_path = os.path.abspath(\n",
    "    os.path.join(notebook_dir, \"../../../../..\")\n",
    ")\n",
    "\n",
    "# Add the resolved path to sys.path if it's not already included\n",
    "if os.path.isdir(features_path) and features_path not in sys.path:\n",
    "    sys.path.append(features_path)\n",
    "\n",
    "from features import generate_xtb_features_dxtb\n",
    "from util.utilities import get_unit_conversion\n",
    "\n",
    "import dxtb\n",
    "dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "\n",
    "class TransitionBatchDataset(IterableDataset):\n",
    "    def __init__(self, hdf5_path, split=\"val\", batch_size=64, mol_names=None):\n",
    "        super().__init__()\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        self.mol_names = mol_names\n",
    "\n",
    "    def __iter__(self):\n",
    "        with h5py.File(self.hdf5_path, \"r\") as f:\n",
    "            for mol_name in list(self.mol_names or f[f\"{self.split}\"].keys()):\n",
    "                mol_group = f[f\"{self.split}/{mol_name}\"]\n",
    "                for rxn_name in mol_group.keys():\n",
    "                    rxn_group = mol_group[rxn_name]\n",
    "                    positions = rxn_group[\"positions\"]\n",
    "                    zs = rxn_group[\"atomic_numbers\"][()]\n",
    "                    n_samples = len(positions)\n",
    "\n",
    "                    for i in range(0, n_samples, self.batch_size):\n",
    "                        pos_batch = torch.tensor(positions[i:i+self.batch_size], **dd) * get_unit_conversion(\"angstrom\", \"bohr\")\n",
    "                        pos_batch.requires_grad_(True)\n",
    "                        z_batch = torch.tensor([zs] * len(pos_batch), device=dd[\"device\"])  # [B, N]\n",
    "                        yield {\n",
    "                            \"mol_name\": mol_name,\n",
    "                            \"rxn_name\": rxn_name,\n",
    "                            \"z\": z_batch,\n",
    "                            \"pos\": pos_batch,\n",
    "                            \"batch_size\": len(pos_batch)\n",
    "                        }\n",
    "\n",
    "# Create dataset + dataloader\n",
    "dataset = TransitionBatchDataset(\n",
    "    hdf5_path=\"../../../../../data/Transition1x/data/transition1x.h5\",\n",
    "    batch_size=64,\n",
    "    mol_names=None\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=None)\n",
    "\n",
    "# Wrap in tqdm and track sample count\n",
    "sample_count = 0\n",
    "pbar = tqdm(dataloader, desc=\"Processing\", unit=\" datapoints\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for batch in pbar:\n",
    "    i += 1\n",
    "    # if i <53:\n",
    "    #     continue\n",
    "    sample_count += batch[\"batch_size\"]\n",
    "    pbar.set_description(f\"{batch['mol_name']}/{batch['rxn_name']}\")\n",
    "    pbar.set_postfix(total=sample_count)\n",
    "    \n",
    "    # print(f\"z {batch['z']}\")\n",
    "    # print(f\"pos {batch['pos']}\")\n",
    "\n",
    "    # DXTB CALC\n",
    "    dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "    opts = {\"scf_mode\": \"full\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "    batch_size = batch['z'].shape[0]\n",
    "    charges = torch.full((batch_size,), 0, **dd)\n",
    "    spin = torch.full((batch_size,), 0, **dd)\n",
    "\n",
    "    calc = dxtb.Calculator(batch['z'], dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "    e = calc.get_energy(batch['pos'], chrg=charges, spin=spin, scf_charges=None)\n",
    "    forces = torch.autograd.grad(sum(e), batch['pos'], retain_graph=True)[0]\n",
    "    \n",
    "\n",
    "    # Features calc\n",
    "    res = generate_xtb_features_dxtb(\n",
    "        batch[\"z\"],\n",
    "        batch[\"pos\"],\n",
    "        charge=charges,\n",
    "        spin=spin,\n",
    "        res_ks=[\"energy\", \"forces\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175aa5cb",
   "metadata": {},
   "source": [
    "# Function in notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8484db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_xtb_features_dxtb(\n",
    "        element_numbers,\n",
    "        coordinates,\n",
    "        charge=0,\n",
    "        spin=0,\n",
    "        res_ks=[\"F\", \"P\", \"S\", \"H\", \"energy\", \"forces\", \"scf_charges\", \"drv\"],\n",
    "        ):\n",
    "    \n",
    "\n",
    "    dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "    opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "    calc = dxtb.Calculator(element_numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "    energy = calc.get_energy(coordinates, chrg=charge, spin=spin)\n",
    "    forces = -torch.autograd.grad(energy.sum(), coordinates, retain_graph=True)[0]\n",
    "\n",
    "    return energy, forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb81d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 0 datapoints [00:00, ? datapoints/s]/tmp/ipykernel_399328/3623958620.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647429097/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  z_batch = torch.tensor([zs] * len(pos_batch), device=dd[\"device\"])  # [B, N]\n",
      "C2H2N2O/rxn2091: : 0 datapoints [00:00, ? datapoints/s, total=64]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_xtb_features_dxtb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     78\u001b[39m forces = torch.autograd.grad(\u001b[38;5;28msum\u001b[39m(e), batch[\u001b[33m'\u001b[39m\u001b[33mpos\u001b[39m\u001b[33m'\u001b[39m], retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Features calc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m res = \u001b[43mgenerate_xtb_features_dxtb\u001b[49m(\n\u001b[32m     83\u001b[39m     batch[\u001b[33m\"\u001b[39m\u001b[33mz\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     84\u001b[39m     batch[\u001b[33m\"\u001b[39m\u001b[33mpos\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     85\u001b[39m     charge=charges,\n\u001b[32m     86\u001b[39m     spin=spin,\n\u001b[32m     87\u001b[39m     res_ks=[\u001b[33m\"\u001b[39m\u001b[33menergy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mforces\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     88\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_xtb_features_dxtb' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dxtb\n",
    "\n",
    "dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "\n",
    "class TransitionBatchDataset(IterableDataset):\n",
    "    def __init__(self, hdf5_path, split=\"val\", batch_size=64, mol_names=None):\n",
    "        super().__init__()\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        self.mol_names = mol_names\n",
    "\n",
    "    def __iter__(self):\n",
    "        with h5py.File(self.hdf5_path, \"r\") as f:\n",
    "            for mol_name in list(self.mol_names or f[f\"{self.split}\"].keys()):\n",
    "                mol_group = f[f\"{self.split}/{mol_name}\"]\n",
    "                for rxn_name in mol_group.keys():\n",
    "                    rxn_group = mol_group[rxn_name]\n",
    "                    positions = rxn_group[\"positions\"]\n",
    "                    zs = rxn_group[\"atomic_numbers\"][()]\n",
    "                    n_samples = len(positions)\n",
    "\n",
    "                    for i in range(0, n_samples, self.batch_size):\n",
    "                        pos_batch = torch.tensor(positions[i:i+self.batch_size], **dd) / 0.529177 # [A] -> [Bohr]\n",
    "                        pos_batch.requires_grad_(True)\n",
    "                        z_batch = torch.tensor([zs] * len(pos_batch), device=dd[\"device\"])  # [B, N]\n",
    "                        yield {\n",
    "                            \"mol_name\": mol_name,\n",
    "                            \"rxn_name\": rxn_name,\n",
    "                            \"z\": z_batch,\n",
    "                            \"pos\": pos_batch,\n",
    "                            \"batch_size\": len(pos_batch)\n",
    "                        }\n",
    "\n",
    "# Create dataset + dataloader\n",
    "dataset = TransitionBatchDataset(\n",
    "    hdf5_path=\"../../../../../data/Transition1x/data/transition1x.h5\",\n",
    "    batch_size=64,\n",
    "    mol_names=None\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=None)\n",
    "\n",
    "# Wrap in tqdm and track sample count\n",
    "sample_count = 0\n",
    "pbar = tqdm(dataloader, desc=\"Processing\", unit=\" datapoints\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for batch in pbar:\n",
    "    i += 1\n",
    "    # if i <53:\n",
    "    #     continue\n",
    "    sample_count += batch[\"batch_size\"]\n",
    "    pbar.set_description(f\"{batch['mol_name']}/{batch['rxn_name']}\")\n",
    "    pbar.set_postfix(total=sample_count)\n",
    "    \n",
    "    # print(f\"z {batch['z']}\")\n",
    "    # print(f\"pos {batch['pos']}\")\n",
    "\n",
    "    # DXTB CALC\n",
    "    dd = {\"dtype\": torch.float32, \"device\": torch.device(\"cuda:0\")}\n",
    "    opts = {\"scf_mode\": \"full\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "    batch_size = batch['z'].shape[0]\n",
    "    charges = torch.full((batch_size,), 0, **dd)\n",
    "    spin = torch.full((batch_size,), 0, **dd)\n",
    "\n",
    "    calc = dxtb.Calculator(batch['z'], dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "\n",
    "    e = calc.get_energy(batch['pos'], chrg=charges, spin=spin)\n",
    "    forces = torch.autograd.grad(sum(e), batch['pos'], retain_graph=True)[0]\n",
    "    \n",
    "\n",
    "    # Features calc\n",
    "    res = generate_xtb_features_dxtb(\n",
    "        batch[\"z\"],\n",
    "        batch[\"pos\"],\n",
    "        charge=charges,\n",
    "        spin=spin,\n",
    "        res_ks=[\"energy\", \"forces\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90995609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbnet-ete",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
