{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c574e3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_star requires_grad? False\n",
      "F_star grad_fn: None\n",
      "\n",
      "[Case 1] No reconnection → autograd fails: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n",
      "[Case 2] After reconnecting via F_star * x:\n",
      "∂E/∂x = 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Dummy fixed point function: F = f(F, x)\n",
    "def fixed_point_map(F, x):\n",
    "    # F* = x + sin(F)\n",
    "    return x + torch.sin(F)\n",
    "\n",
    "# Fake equilibrium solver (just 1 iteration for simplicity)\n",
    "def equilibrium(f, guess, params):\n",
    "    x = params[0].detach()\n",
    "    F = f(guess, x)\n",
    "    return F  # simulate that this came from a rootfinder with no tracking\n",
    "\n",
    "# Inputs\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "F_guess = torch.tensor([0.0]).detach()  # doesn't require grad\n",
    "\n",
    "# Run \"SCF\" solver\n",
    "F_star = equilibrium(fixed_point_map, F_guess, params=[x])\n",
    "print(\"F_star requires_grad?\", F_star.requires_grad)\n",
    "print(\"F_star grad_fn:\", F_star.grad_fn)\n",
    "\n",
    "# --- Case 1: E(F_star) directly → fails\n",
    "E1 = (F_star**2).sum()\n",
    "try:\n",
    "    torch.autograd.grad(E1, x)\n",
    "except RuntimeError as e:\n",
    "    print(\"\\n[Case 1] No reconnection → autograd fails:\", e)\n",
    "\n",
    "# --- Case 2: reconnect F_star to x\n",
    "F_connected = F_star + x  # reconnect using a dummy op\n",
    "E2 = (F_connected**2).sum()\n",
    "grad2 = torch.autograd.grad(E2, x)[0]\n",
    "print(\"\\n[Case 2] After reconnecting via F_star * x:\")\n",
    "print(\"∂E/∂x =\", grad2.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92b16944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_star requires_grad? False\n",
      "F_star grad_fn: None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# --- Case: E(F_star)\u001b[39;00m\n\u001b[32m     42\u001b[39m E = (F_star**\u001b[32m2\u001b[39m).sum()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m grad_x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGradient ∂E/∂x =\u001b[39m\u001b[33m\"\u001b[39m, grad_x.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dxtb/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    492\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    493\u001b[39m         grad_outputs_\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    507\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    508\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    509\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dxtb/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define custom autograd Function\n",
    "class CustomEquilibrium(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, F_guess, x):\n",
    "        # Simulate fixed point iteration: F* = x + sin(F)\n",
    "        with torch.no_grad():\n",
    "            F = x + torch.sin(F_guess)\n",
    "        ctx.save_for_backward(F, x)\n",
    "        return F\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        F, x = ctx.saved_tensors\n",
    "\n",
    "        # f(F, x) = x + sin(F)\n",
    "        # ∂f/∂F = cos(F)\n",
    "        # ∂f/∂x = 1\n",
    "\n",
    "        dfdF = torch.cos(F)\n",
    "        dfdx = torch.ones_like(x)\n",
    "\n",
    "        # Apply implicit differentiation:\n",
    "        # dF*/dx = (I - ∂f/∂F)^(-1) * ∂f/∂x\n",
    "        # Since everything is scalar here, it's just:\n",
    "        dFdx = 1.0 / (1.0 - dfdF) * dfdx\n",
    "\n",
    "        grad_x = grad_output * dFdx # dsmthg/dF * dF/dx\n",
    "        return None, grad_x  # no grad for F_guess\n",
    "\n",
    "# Inputs\n",
    "x = torch.tensor([2.0], requires_grad=True).detach()\n",
    "F_guess = torch.tensor([0.0]).detach()  # doesn't require grad\n",
    "\n",
    "# Run equilibrium with custom backward\n",
    "F_star = CustomEquilibrium.apply(F_guess, x)\n",
    "print(\"F_star requires_grad?\", F_star.requires_grad)\n",
    "print(\"F_star grad_fn:\", F_star.grad_fn)\n",
    "\n",
    "# --- Case: E(F_star)\n",
    "E = (F_star**2).sum()\n",
    "grad_x = torch.autograd.grad(E, x)[0]\n",
    "print(\"\\nGradient ∂E/∂x =\", grad_x.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dxtb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
