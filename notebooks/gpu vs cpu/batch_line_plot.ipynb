{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of carbon atoms in alkane_9_carbons: 9\n",
      "Nb of atoms: 29\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import h5py\n",
    "N_Cs = 9\n",
    "\n",
    "with h5py.File('../dxtb/dxtb-gpu/gpu-cpu_analysis/rdkit/alkanes_data_500.hdf5', 'r') as f:\n",
    "    for mol_name, data in f.items():\n",
    "        if mol_name == f\"alkane_{N_Cs}_carbons\":\n",
    "            atomic_numbers = data['atomic_numbers'][:]\n",
    "            coordinates = data['coordinates'][:]\n",
    "\n",
    "print(f\"Number of carbon atoms in {mol_name}: {N_Cs}\")\n",
    "print(f\"Nb of atoms: {len(atomic_numbers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cuda:0\n",
      "iscomplex: False\n",
      "\n",
      "\n",
      "Timings\n",
      "-------\n",
      "\n",
      "\u001b[1mObjective                Time (s)        % Total\u001b[0m\n",
      "------------------------------------------------\n",
      "\u001b[1mClassicals                  0.008           0.22\u001b[0m\n",
      " - DispersionD3        \u001b[37m     0.003          40.50\u001b[0m\n",
      " - Repulsion           \u001b[37m     0.001           8.88\u001b[0m\n",
      " - Halogen             \u001b[37m     0.004          49.90\u001b[0m\n",
      "\u001b[1mIntegrals                   0.189           5.35\u001b[0m\n",
      " - Overlap             \u001b[37m     0.187          99.07\u001b[0m\n",
      " - Core Hamiltonian    \u001b[37m     0.002           0.92\u001b[0m\n",
      "\u001b[1mSCF                         3.328          94.21\u001b[0m\n",
      " - Interaction Cache   \u001b[37m     0.001           0.02\u001b[0m\n",
      " - Potential           \u001b[37m     3.302          99.21\u001b[0m\n",
      " - Fock build          \u001b[37m     0.001           0.02\u001b[0m\n",
      " - Diagonalize         \u001b[37m     3.279          98.53\u001b[0m\n",
      " - Density             \u001b[37m     0.005           0.16\u001b[0m\n",
      " - Charges             \u001b[37m     0.003           0.10\u001b[0m\n",
      "\u001b[1meigh                        3.244          91.85\u001b[0m\n",
      "------------------------------------------------\n",
      "Sum                    \u001b[37m     6.769         191.63\u001b[0m\n",
      "\u001b[1mTotal                       3.532         100.00\u001b[0m\n",
      "Device: cpu\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "shape matrix to be diagonalized: torch.Size([64, 76, 76]), device: cpu\n",
      "iscomplex: False\n",
      "\n",
      "\n",
      "Timings\n",
      "-------\n",
      "\n",
      "\u001b[1mObjective                Time (s)        % Total\u001b[0m\n",
      "------------------------------------------------\n",
      "\u001b[1mClassicals                  0.014           2.15\u001b[0m\n",
      " - Repulsion           \u001b[37m     0.001           9.58\u001b[0m\n",
      " - Halogen             \u001b[37m     0.002          13.53\u001b[0m\n",
      " - DispersionD3        \u001b[37m     0.010          76.53\u001b[0m\n",
      "\u001b[1mIntegrals                   0.143          22.57\u001b[0m\n",
      " - Overlap             \u001b[37m     0.137          95.97\u001b[0m\n",
      " - Core Hamiltonian    \u001b[37m     0.006           4.02\u001b[0m\n",
      "\u001b[1mSCF                         0.472          74.55\u001b[0m\n",
      " - Interaction Cache   \u001b[37m     0.002           0.46\u001b[0m\n",
      " - Potential           \u001b[37m     0.452          95.71\u001b[0m\n",
      " - Fock build          \u001b[37m     0.009           1.87\u001b[0m\n",
      " - Diagonalize         \u001b[37m     0.418          88.63\u001b[0m\n",
      " - Density             \u001b[37m     0.009           1.98\u001b[0m\n",
      " - Charges             \u001b[37m     0.005           1.11\u001b[0m\n",
      "\u001b[1meigh                        0.301          47.58\u001b[0m\n",
      "------------------------------------------------\n",
      "Sum                    \u001b[37m     0.929         146.86\u001b[0m\n",
      "\u001b[1mTotal                       0.633         100.00\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import dxtb\n",
    "from dxtb._src.typing import DD\n",
    "import torch\n",
    "import ase.build.molecule as molecule\n",
    "\n",
    "opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for device in [\"cuda:0\", \"cpu\"]:\n",
    "    print(f\"Device: {device}\")\n",
    "    dd = {\"dtype\": torch.float64, \"device\": torch.device(device)}\n",
    "    numbers = torch.tensor(atomic_numbers, device= dd[\"device\"], dtype = torch.int32)\n",
    "    positions = torch.tensor(coordinates, device = dd['device'], dtype = dd['dtype'])\n",
    "    numbers = torch.stack([numbers] * batch_size)\n",
    "    positions = torch.stack([positions] * batch_size).requires_grad_()\n",
    "    charges = torch.zeros(((batch_size,)), device=dd['device'], dtype=dd['dtype'])\n",
    "    \n",
    "    # dxtb.timer.reset()\n",
    "    calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts, timer=True)\n",
    "    dxtb.timer.reset()\n",
    "    e = calc.get_energy(positions, chrg=charges)\n",
    "    # dxtb.timer.start(\"Forces autograd\")\n",
    "    # forces = torch.autograd.grad(sum(e), positions, retain_graph=True)[0]\n",
    "    # dxtb.timer.stop(\"Forces autograd\")\n",
    "    dxtb.timer.print(v=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dxtb\n",
    "from dxtb._src.typing import DD\n",
    "import ase.build.molecule as molecule\n",
    "import torch.profiler\n",
    "\n",
    "opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "batch_size = 100\n",
    "\n",
    "for device in [\"cpu\", \"cuda:0\"]:\n",
    "    dd = {\"dtype\": torch.float64, \"device\": torch.device(device)}\n",
    "    numbers = torch.tensor(atomic_numbers, device=dd[\"device\"], dtype=torch.int32)\n",
    "    positions = torch.tensor(coordinates, device=dd[\"device\"], dtype=dd[\"dtype\"])\n",
    "    numbers = torch.stack([numbers] * batch_size)\n",
    "    positions = torch.stack([positions] * batch_size).requires_grad_()\n",
    "    charges = torch.zeros((batch_size,), device=dd[\"device\"], dtype=dd[\"dtype\"])\n",
    "\n",
    "    torch.cuda.synchronize() if \"cuda\" in device else None\n",
    "    dxtb.timer.reset()\n",
    "    calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts, timer=True)\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA\n",
    "        ] if \"cuda\" in device else [torch.profiler.ProfilerActivity.CPU],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        e = calc.get_energy(positions, chrg=charges)\n",
    "        dxtb.timer.start(\"Forces autograd\")\n",
    "        forces = torch.autograd.grad(sum(e), positions, retain_graph=True)[0]\n",
    "        dxtb.timer.stop(\"Forces autograd\")\n",
    "        torch.cuda.synchronize() if \"cuda\" in device else None\n",
    "\n",
    "    dxtb.timer.print(v=0)\n",
    "    print(f\"\\n--- PyTorch Profiler Summary ({device}) ---\")\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\" if \"cuda\" in device else \"cpu_time_total\", row_limit=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dxtb\n",
    "from dxtb._src.typing import DD\n",
    "import torch\n",
    "import ase.build.molecule as molecule\n",
    "\n",
    "opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 0}\n",
    "\n",
    "for device in [\"cpu\", \"cuda:0\"]:\n",
    "    dd = {\"dtype\": torch.float64, \"device\": torch.device(device)}\n",
    "    numbers = torch.tensor(atomic_numbers, device= dd[\"device\"], dtype = torch.int64)\n",
    "    positions = torch.tensor(coordinates, device = dd['device'], dtype = torch.float64).requires_grad_()\n",
    "\n",
    "    dxtb.timer.reset()\n",
    "    calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "    e = calc.get_energy(positions)\n",
    "    forces = calc.get_forces(positions)\n",
    "    dxtb.timer.print(v=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dxtb\n",
    "from dxtb._src.typing import DD\n",
    "import torch\n",
    "import ase.build.molecule as molecule\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup\n",
    "opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 1}\n",
    "\n",
    "batch_sizes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "timings = {\"cpu\": [], \"cuda:0\": []}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for device in [\"cpu\", \"cuda:0\"]:\n",
    "        dd: DD = {\"dtype\": torch.float64, \"device\": torch.device(device)}\n",
    "        numbers = torch.tensor(atomic_numbers, device=dd[\"device\"], dtype=torch.int32)\n",
    "        positions = torch.tensor(coordinates, device=dd[\"device\"], dtype=dd[\"dtype\"])\n",
    "\n",
    "        numbers = torch.stack([numbers] * batch_size)\n",
    "        positions = torch.stack([positions] * batch_size)\n",
    "        charges = torch.zeros((batch_size,), device=dd[\"device\"], dtype=dd[\"dtype\"])\n",
    "\n",
    "        dxtb.timer.reset()\n",
    "        calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "        e = calc.get_energy(positions, chrg=charges)\n",
    "        dxtb.timer.print(v=5)\n",
    "        total = dxtb.timer.timers[\"total\"].elapsed_time\n",
    "        timings[device].append(total)\n",
    "        print(f\"[{device}] Batch {batch_size} → {total:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for device in timings:\n",
    "    actual_times = timings[device]\n",
    "    \n",
    "    # Plot actual timings\n",
    "    plt.plot(batch_sizes, actual_times, label=f\"{device}\", marker='o', linestyle='-')\n",
    "    \n",
    "    # Get anchor time at batch size 2\n",
    "    anchor_batch_size = 4\n",
    "    anchor_idx = batch_sizes.index(anchor_batch_size)\n",
    "    anchor_time = actual_times[anchor_idx]\n",
    "    \n",
    "    # Generate linear scaling line\n",
    "    linear_times = [anchor_time * (b / anchor_batch_size) for b in batch_sizes]\n",
    "    plt.plot(batch_sizes, linear_times, linestyle='--', label=f\"{device} (linear)\", alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Total Time (s)\")\n",
    "plt.title(\"dxtb Total Time vs. Batch Size with Linear Scaling Baseline\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dxtb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
