{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of carbon atoms in alkane_9_carbons: 9\n",
      "Nb of atoms: 29\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import h5py\n",
    "N_Cs = 9\n",
    "\n",
    "with h5py.File('../dxtb/dxtb-gpu/gpu-cpu_analysis/rdkit/alkanes_data_500.hdf5', 'r') as f:\n",
    "    for mol_name, data in f.items():\n",
    "        if mol_name == f\"alkane_{N_Cs}_carbons\":\n",
    "            atomic_numbers = data['atomic_numbers'][:]\n",
    "            coordinates = data['coordinates'][:]\n",
    "\n",
    "print(f\"Number of carbon atoms in {mol_name}: {N_Cs}\")\n",
    "print(f\"Nb of atoms: {len(atomic_numbers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device: cuda:0\n",
      "\n",
      "\n",
      "Timings\n",
      "-------\n",
      "\n",
      "\u001b[1mObjective                Time (s)        % Total\u001b[0m\n",
      "------------------------------------------------\n",
      "\u001b[1mClassicals                  0.011           1.55\u001b[0m\n",
      " - Repulsion           \u001b[37m     0.001           6.43\u001b[0m\n",
      " - DispersionD3        \u001b[37m     0.003          27.53\u001b[0m\n",
      " - Halogen             \u001b[37m     0.007          65.63\u001b[0m\n",
      "\u001b[1mIntegrals                   0.504          71.90\u001b[0m\n",
      " - Overlap             \u001b[37m     0.502          99.65\u001b[0m\n",
      " - Core Hamiltonian    \u001b[37m     0.002           0.35\u001b[0m\n",
      "\u001b[1mSCF                         0.018           2.63\u001b[0m\n",
      " - Interaction Cache   \u001b[37m     0.001           3.94\u001b[0m\n",
      " - Potential           \u001b[37m     0.012          64.70\u001b[0m\n",
      " - Fock build          \u001b[37m     0.000           0.59\u001b[0m\n",
      " - Diagonalize         \u001b[37m     0.008          43.24\u001b[0m\n",
      " - Density             \u001b[37m     0.001           3.39\u001b[0m\n",
      " - Charges             \u001b[37m     0.001           2.95\u001b[0m\n",
      "\u001b[1mcupy_eigh                   0.004           0.52\u001b[0m\n",
      "\u001b[1mForces autograd             0.157          22.46\u001b[0m\n",
      "------------------------------------------------\n",
      "Sum                    \u001b[37m     0.694          99.05\u001b[0m\n",
      "\u001b[1mTotal                       0.700         100.00\u001b[0m\n",
      "\n",
      "Device: cpu\n",
      "\n",
      "\n",
      "Timings\n",
      "-------\n",
      "\n",
      "\u001b[1mObjective                Time (s)        % Total\u001b[0m\n",
      "------------------------------------------------\n",
      "\u001b[1mClassicals                  0.009           1.28\u001b[0m\n",
      " - DispersionD3        \u001b[37m     0.005          58.28\u001b[0m\n",
      " - Halogen             \u001b[37m     0.003          29.71\u001b[0m\n",
      " - Repulsion           \u001b[37m     0.001          11.55\u001b[0m\n",
      "\u001b[1mIntegrals                   0.439          60.66\u001b[0m\n",
      " - Overlap             \u001b[37m     0.434          99.02\u001b[0m\n",
      " - Core Hamiltonian    \u001b[37m     0.004           0.97\u001b[0m\n",
      "\u001b[1mSCF                         0.098          13.54\u001b[0m\n",
      " - Interaction Cache   \u001b[37m     0.002           1.58\u001b[0m\n",
      " - Potential           \u001b[37m     0.091          93.11\u001b[0m\n",
      " - Fock build          \u001b[37m     0.001           1.13\u001b[0m\n",
      " - Diagonalize         \u001b[37m     0.086          88.28\u001b[0m\n",
      " - Density             \u001b[37m     0.001           1.07\u001b[0m\n",
      " - Charges             \u001b[37m     0.001           0.88\u001b[0m\n",
      "\u001b[1mtorch_eigh                  0.060           8.35\u001b[0m\n",
      "\u001b[1mForces autograd             0.163          22.54\u001b[0m\n",
      "------------------------------------------------\n",
      "Sum                    \u001b[37m     0.769         106.37\u001b[0m\n",
      "\u001b[1mTotal                       0.723         100.00\u001b[0m\n",
      "\n",
      "[Comparison]\n",
      "GPU energy: -3.531986e+00\n",
      "CPU energy: -3.531616e+00\n",
      "Max energy diff: 3.726482e-04\n",
      "Max forces diff: 1.895428e-05\n"
     ]
    }
   ],
   "source": [
    "import dxtb\n",
    "from dxtb._src.typing import DD\n",
    "import torch\n",
    "from ase.build import molecule\n",
    "\n",
    "opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 1, \"int_driver\": \"libcint\", \"maxiter\": 1}\n",
    "batch_size = 128\n",
    "results = {}\n",
    "\n",
    "for device in [\"cuda:0\", \"cpu\"]:\n",
    "    print(f\"\\nDevice: {device}\")\n",
    "    dd = {\"dtype\": torch.float32, \"device\": torch.device(device)}\n",
    "    numbers = torch.tensor(atomic_numbers, device=dd[\"device\"], dtype=torch.int32)\n",
    "    positions = torch.tensor(coordinates, device=dd[\"device\"], dtype=dd[\"dtype\"])\n",
    "    numbers = torch.stack([numbers] * batch_size)\n",
    "    positions = torch.stack([positions] * batch_size).requires_grad_()\n",
    "    charges = torch.zeros((batch_size,), device=dd[\"device\"], dtype=dd[\"dtype\"])\n",
    "\n",
    "    calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts, timer=True)\n",
    "    \n",
    "    dxtb.timer.reset()\n",
    "    e = calc.get_energy(positions, chrg=charges)\n",
    "    dxtb.timer.start(\"Forces autograd\")\n",
    "    forces = torch.autograd.grad(sum(e), positions, retain_graph=True)[0]\n",
    "    dxtb.timer.stop(\"Forces autograd\")\n",
    "    dxtb.timer.print(v=0)\n",
    "\n",
    "    results[device] = {\n",
    "        \"energy\": e.detach().cpu(),\n",
    "        \"forces\": forces.detach().cpu()\n",
    "    }\n",
    "\n",
    "# Compare results\n",
    "energy_diff = (results[\"cuda:0\"][\"energy\"] - results[\"cpu\"][\"energy\"]).abs().max()\n",
    "forces_diff = (results[\"cuda:0\"][\"forces\"] - results[\"cpu\"][\"forces\"]).abs().max()\n",
    "\n",
    "print(f\"\\n[Comparison]\")\n",
    "print(f\"GPU energy: {results['cuda:0']['energy'].mean().item():.6e}\")\n",
    "print(f\"CPU energy: {results['cpu']['energy'].mean().item():.6e}\")\n",
    "print(f\"Max energy diff: {energy_diff.item():.6e}\")\n",
    "print(f\"Max forces diff: {forces_diff.item():.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dxtb\n",
    "from dxtb._src.typing import DD\n",
    "import ase.build.molecule as molecule\n",
    "import torch.profiler\n",
    "\n",
    "opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 2, \"int_driver\": \"libcint\"}\n",
    "batch_size = 100\n",
    "\n",
    "for device in [\"cpu\", \"cuda:0\"]:\n",
    "    dd = {\"dtype\": torch.float64, \"device\": torch.device(device)}\n",
    "    numbers = torch.tensor(atomic_numbers, device=dd[\"device\"], dtype=torch.int32)\n",
    "    positions = torch.tensor(coordinates, device=dd[\"device\"], dtype=dd[\"dtype\"])\n",
    "    numbers = torch.stack([numbers] * batch_size)\n",
    "    positions = torch.stack([positions] * batch_size).requires_grad_()\n",
    "    charges = torch.zeros((batch_size,), device=dd[\"device\"], dtype=dd[\"dtype\"])\n",
    "\n",
    "    torch.cuda.synchronize() if \"cuda\" in device else None\n",
    "    dxtb.timer.reset()\n",
    "    calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts, timer=True)\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA\n",
    "        ] if \"cuda\" in device else [torch.profiler.ProfilerActivity.CPU],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        e = calc.get_energy(positions, chrg=charges)\n",
    "        dxtb.timer.start(\"Forces autograd\")\n",
    "        forces = torch.autograd.grad(sum(e), positions, retain_graph=True)[0]\n",
    "        dxtb.timer.stop(\"Forces autograd\")\n",
    "        torch.cuda.synchronize() if \"cuda\" in device else None\n",
    "\n",
    "    dxtb.timer.print(v=0)\n",
    "    print(f\"\\n--- PyTorch Profiler Summary ({device}) ---\")\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\" if \"cuda\" in device else \"cpu_time_total\", row_limit=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dxtb\n",
    "from dxtb._src.typing import DD\n",
    "import torch\n",
    "import ase.build.molecule as molecule\n",
    "\n",
    "opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 0}\n",
    "\n",
    "for device in [\"cpu\", \"cuda:0\"]:\n",
    "    dd = {\"dtype\": torch.float64, \"device\": torch.device(device)}\n",
    "    numbers = torch.tensor(atomic_numbers, device= dd[\"device\"], dtype = torch.int64)\n",
    "    positions = torch.tensor(coordinates, device = dd['device'], dtype = torch.float64).requires_grad_()\n",
    "\n",
    "    dxtb.timer.reset()\n",
    "    calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "    e = calc.get_energy(positions)\n",
    "    forces = calc.get_forces(positions)\n",
    "    dxtb.timer.print(v=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dxtb\n",
    "from dxtb._src.typing import DD\n",
    "import torch\n",
    "import ase.build.molecule as molecule\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup\n",
    "opts = {\"scf_mode\": \"implicit\", \"batch_mode\": 1}\n",
    "\n",
    "batch_sizes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "timings = {\"cpu\": [], \"cuda:0\": []}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for device in [\"cpu\", \"cuda:0\"]:\n",
    "        dd: DD = {\"dtype\": torch.float64, \"device\": torch.device(device)}\n",
    "        numbers = torch.tensor(atomic_numbers, device=dd[\"device\"], dtype=torch.int32)\n",
    "        positions = torch.tensor(coordinates, device=dd[\"device\"], dtype=dd[\"dtype\"])\n",
    "\n",
    "        numbers = torch.stack([numbers] * batch_size)\n",
    "        positions = torch.stack([positions] * batch_size)\n",
    "        charges = torch.zeros((batch_size,), device=dd[\"device\"], dtype=dd[\"dtype\"])\n",
    "\n",
    "        dxtb.timer.reset()\n",
    "        calc = dxtb.Calculator(numbers, dxtb.GFN1_XTB, **dd, opts=opts)\n",
    "        e = calc.get_energy(positions, chrg=charges)\n",
    "        dxtb.timer.print(v=5)\n",
    "        total = dxtb.timer.timers[\"total\"].elapsed_time\n",
    "        timings[device].append(total)\n",
    "        print(f\"[{device}] Batch {batch_size} → {total:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for device in timings:\n",
    "    actual_times = timings[device]\n",
    "    \n",
    "    # Plot actual timings\n",
    "    plt.plot(batch_sizes, actual_times, label=f\"{device}\", marker='o', linestyle='-')\n",
    "    \n",
    "    # Get anchor time at batch size 2\n",
    "    anchor_batch_size = 4\n",
    "    anchor_idx = batch_sizes.index(anchor_batch_size)\n",
    "    anchor_time = actual_times[anchor_idx]\n",
    "    \n",
    "    # Generate linear scaling line\n",
    "    linear_times = [anchor_time * (b / anchor_batch_size) for b in batch_sizes]\n",
    "    plt.plot(batch_sizes, linear_times, linestyle='--', label=f\"{device} (linear)\", alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Total Time (s)\")\n",
    "plt.title(\"dxtb Total Time vs. Batch Size with Linear Scaling Baseline\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dxtb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
